{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1087874\n",
      "25555\n",
      "['aaaaarrrgh', 'aaaah', 'aaaargh', 'aaah', 'aaahpain', 'aaargh', 'aah', 'aall', 'aargh', 'ab']\n"
     ]
    }
   ],
   "source": [
    "with open('hp.txt', 'r', encoding='UTF-8') as f:\n",
    "    train_data = f.read()\n",
    "\n",
    "# Split train_data into words\n",
    "train_data = train_data.lower()\n",
    "train_data = re.sub(r'[^\\w\\s]', '', train_data)\n",
    "train_data = re.findall(r'\\w+|[^\\s\\w]', train_data)\n",
    "words_available = sorted(list(set(train_data)))\n",
    "stoi = {word: i for i, word in enumerate(words_available)}\n",
    "itos = {i: word for i, word in enumerate(words_available)}\n",
    "train_data = [stoi[word] for word in train_data]\n",
    "\n",
    "print(len(train_data))\n",
    "# Only use the first 10000 words\n",
    "# train_data = train_data[:700000]\n",
    "\n",
    "vocab_size = len(words_available)\n",
    "print(vocab_size)\n",
    "# Show example of words\n",
    "print(words_available[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+1:i+seq_length+1]\n",
    "        if len(target) < seq_length or len(seq) < seq_length:\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return sequences, targets\n",
    "\n",
    "seq_length = 30\n",
    "sequences, targets = create_sequences(train_data, seq_length)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "dataset = TextDataset(sequences, targets)\n",
    "batch_size = 256\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 14:24:26,345 : INFO : collecting all words and their counts\n",
      "2024-10-22 14:24:26,347 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-10-22 14:24:26,395 : INFO : PROGRESS: at sentence #10000, processed 300000 words, keeping 1933 word types\n",
      "2024-10-22 14:24:26,431 : INFO : PROGRESS: at sentence #20000, processed 600000 words, keeping 3058 word types\n",
      "2024-10-22 14:24:26,466 : INFO : PROGRESS: at sentence #30000, processed 900000 words, keeping 3803 word types\n",
      "2024-10-22 14:24:26,500 : INFO : PROGRESS: at sentence #40000, processed 1200000 words, keeping 4493 word types\n",
      "2024-10-22 14:24:26,534 : INFO : PROGRESS: at sentence #50000, processed 1500000 words, keeping 5017 word types\n",
      "2024-10-22 14:24:26,569 : INFO : PROGRESS: at sentence #60000, processed 1800000 words, keeping 5467 word types\n",
      "2024-10-22 14:24:26,603 : INFO : PROGRESS: at sentence #70000, processed 2100000 words, keeping 5861 word types\n",
      "2024-10-22 14:24:26,637 : INFO : PROGRESS: at sentence #80000, processed 2400000 words, keeping 6291 word types\n",
      "2024-10-22 14:24:26,671 : INFO : PROGRESS: at sentence #90000, processed 2700000 words, keeping 6817 word types\n",
      "2024-10-22 14:24:26,705 : INFO : PROGRESS: at sentence #100000, processed 3000000 words, keeping 7265 word types\n",
      "2024-10-22 14:24:26,740 : INFO : PROGRESS: at sentence #110000, processed 3300000 words, keeping 7780 word types\n",
      "2024-10-22 14:24:26,774 : INFO : PROGRESS: at sentence #120000, processed 3600000 words, keeping 8230 word types\n",
      "2024-10-22 14:24:26,808 : INFO : PROGRESS: at sentence #130000, processed 3900000 words, keeping 8587 word types\n",
      "2024-10-22 14:24:26,842 : INFO : PROGRESS: at sentence #140000, processed 4200000 words, keeping 8941 word types\n",
      "2024-10-22 14:24:26,877 : INFO : PROGRESS: at sentence #150000, processed 4500000 words, keeping 9223 word types\n",
      "2024-10-22 14:24:26,911 : INFO : PROGRESS: at sentence #160000, processed 4800000 words, keeping 9470 word types\n",
      "2024-10-22 14:24:26,945 : INFO : PROGRESS: at sentence #170000, processed 5100000 words, keeping 9780 word types\n",
      "2024-10-22 14:24:26,980 : INFO : PROGRESS: at sentence #180000, processed 5400000 words, keeping 10158 word types\n",
      "2024-10-22 14:24:27,015 : INFO : PROGRESS: at sentence #190000, processed 5700000 words, keeping 10461 word types\n",
      "2024-10-22 14:24:27,051 : INFO : PROGRESS: at sentence #200000, processed 6000000 words, keeping 10734 word types\n",
      "2024-10-22 14:24:27,088 : INFO : PROGRESS: at sentence #210000, processed 6300000 words, keeping 10949 word types\n",
      "2024-10-22 14:24:27,125 : INFO : PROGRESS: at sentence #220000, processed 6600000 words, keeping 11243 word types\n",
      "2024-10-22 14:24:27,164 : INFO : PROGRESS: at sentence #230000, processed 6900000 words, keeping 11451 word types\n",
      "2024-10-22 14:24:27,201 : INFO : PROGRESS: at sentence #240000, processed 7200000 words, keeping 11669 word types\n",
      "2024-10-22 14:24:27,238 : INFO : PROGRESS: at sentence #250000, processed 7500000 words, keeping 11860 word types\n",
      "2024-10-22 14:24:27,275 : INFO : PROGRESS: at sentence #260000, processed 7800000 words, keeping 12010 word types\n",
      "2024-10-22 14:24:27,311 : INFO : PROGRESS: at sentence #270000, processed 8100000 words, keeping 12115 word types\n",
      "2024-10-22 14:24:27,347 : INFO : PROGRESS: at sentence #280000, processed 8400000 words, keeping 12400 word types\n",
      "2024-10-22 14:24:27,383 : INFO : PROGRESS: at sentence #290000, processed 8700000 words, keeping 12684 word types\n",
      "2024-10-22 14:24:27,418 : INFO : PROGRESS: at sentence #300000, processed 9000000 words, keeping 13095 word types\n",
      "2024-10-22 14:24:27,453 : INFO : PROGRESS: at sentence #310000, processed 9300000 words, keeping 13375 word types\n",
      "2024-10-22 14:24:27,488 : INFO : PROGRESS: at sentence #320000, processed 9600000 words, keeping 13698 word types\n",
      "2024-10-22 14:24:27,523 : INFO : PROGRESS: at sentence #330000, processed 9900000 words, keeping 13997 word types\n",
      "2024-10-22 14:24:27,558 : INFO : PROGRESS: at sentence #340000, processed 10200000 words, keeping 14239 word types\n",
      "2024-10-22 14:24:27,592 : INFO : PROGRESS: at sentence #350000, processed 10500000 words, keeping 14489 word types\n",
      "2024-10-22 14:24:27,627 : INFO : PROGRESS: at sentence #360000, processed 10800000 words, keeping 14703 word types\n",
      "2024-10-22 14:24:27,661 : INFO : PROGRESS: at sentence #370000, processed 11100000 words, keeping 14936 word types\n",
      "2024-10-22 14:24:27,697 : INFO : PROGRESS: at sentence #380000, processed 11400000 words, keeping 15177 word types\n",
      "2024-10-22 14:24:27,751 : INFO : PROGRESS: at sentence #390000, processed 11700000 words, keeping 15455 word types\n",
      "2024-10-22 14:24:27,788 : INFO : PROGRESS: at sentence #400000, processed 12000000 words, keeping 15695 word types\n",
      "2024-10-22 14:24:27,824 : INFO : PROGRESS: at sentence #410000, processed 12300000 words, keeping 15910 word types\n",
      "2024-10-22 14:24:27,860 : INFO : PROGRESS: at sentence #420000, processed 12600000 words, keeping 16104 word types\n",
      "2024-10-22 14:24:27,895 : INFO : PROGRESS: at sentence #430000, processed 12900000 words, keeping 16296 word types\n",
      "2024-10-22 14:24:27,932 : INFO : PROGRESS: at sentence #440000, processed 13200000 words, keeping 16555 word types\n",
      "2024-10-22 14:24:27,968 : INFO : PROGRESS: at sentence #450000, processed 13500000 words, keeping 16772 word types\n",
      "2024-10-22 14:24:28,004 : INFO : PROGRESS: at sentence #460000, processed 13800000 words, keeping 16926 word types\n",
      "2024-10-22 14:24:28,039 : INFO : PROGRESS: at sentence #470000, processed 14100000 words, keeping 17157 word types\n",
      "2024-10-22 14:24:28,075 : INFO : PROGRESS: at sentence #480000, processed 14400000 words, keeping 17357 word types\n",
      "2024-10-22 14:24:28,112 : INFO : PROGRESS: at sentence #490000, processed 14700000 words, keeping 17530 word types\n",
      "2024-10-22 14:24:28,149 : INFO : PROGRESS: at sentence #500000, processed 15000000 words, keeping 17757 word types\n",
      "2024-10-22 14:24:28,183 : INFO : PROGRESS: at sentence #510000, processed 15300000 words, keeping 17895 word types\n",
      "2024-10-22 14:24:28,218 : INFO : PROGRESS: at sentence #520000, processed 15600000 words, keeping 18103 word types\n",
      "2024-10-22 14:24:28,252 : INFO : PROGRESS: at sentence #530000, processed 15900000 words, keeping 18329 word types\n",
      "2024-10-22 14:24:28,287 : INFO : PROGRESS: at sentence #540000, processed 16200000 words, keeping 18474 word types\n",
      "2024-10-22 14:24:28,321 : INFO : PROGRESS: at sentence #550000, processed 16500000 words, keeping 18600 word types\n",
      "2024-10-22 14:24:28,356 : INFO : PROGRESS: at sentence #560000, processed 16800000 words, keeping 18747 word types\n",
      "2024-10-22 14:24:28,390 : INFO : PROGRESS: at sentence #570000, processed 17100000 words, keeping 18866 word types\n",
      "2024-10-22 14:24:28,424 : INFO : PROGRESS: at sentence #580000, processed 17400000 words, keeping 19012 word types\n",
      "2024-10-22 14:24:28,459 : INFO : PROGRESS: at sentence #590000, processed 17700000 words, keeping 19186 word types\n",
      "2024-10-22 14:24:28,492 : INFO : PROGRESS: at sentence #600000, processed 18000000 words, keeping 19309 word types\n",
      "2024-10-22 14:24:28,527 : INFO : PROGRESS: at sentence #610000, processed 18300000 words, keeping 19471 word types\n",
      "2024-10-22 14:24:28,561 : INFO : PROGRESS: at sentence #620000, processed 18600000 words, keeping 19614 word types\n",
      "2024-10-22 14:24:28,596 : INFO : PROGRESS: at sentence #630000, processed 18900000 words, keeping 19760 word types\n",
      "2024-10-22 14:24:28,630 : INFO : PROGRESS: at sentence #640000, processed 19200000 words, keeping 19880 word types\n",
      "2024-10-22 14:24:28,664 : INFO : PROGRESS: at sentence #650000, processed 19500000 words, keeping 19994 word types\n",
      "2024-10-22 14:24:28,699 : INFO : PROGRESS: at sentence #660000, processed 19800000 words, keeping 20110 word types\n",
      "2024-10-22 14:24:28,734 : INFO : PROGRESS: at sentence #670000, processed 20100000 words, keeping 20254 word types\n",
      "2024-10-22 14:24:28,768 : INFO : PROGRESS: at sentence #680000, processed 20400000 words, keeping 20381 word types\n",
      "2024-10-22 14:24:28,803 : INFO : PROGRESS: at sentence #690000, processed 20700000 words, keeping 20506 word types\n",
      "2024-10-22 14:24:28,837 : INFO : PROGRESS: at sentence #700000, processed 21000000 words, keeping 20647 word types\n",
      "2024-10-22 14:24:28,872 : INFO : PROGRESS: at sentence #710000, processed 21300000 words, keeping 20757 word types\n",
      "2024-10-22 14:24:28,907 : INFO : PROGRESS: at sentence #720000, processed 21600000 words, keeping 20871 word types\n",
      "2024-10-22 14:24:28,944 : INFO : PROGRESS: at sentence #730000, processed 21900000 words, keeping 21056 word types\n",
      "2024-10-22 14:24:28,979 : INFO : PROGRESS: at sentence #740000, processed 22200000 words, keeping 21234 word types\n",
      "2024-10-22 14:24:29,016 : INFO : PROGRESS: at sentence #750000, processed 22500000 words, keeping 21373 word types\n",
      "2024-10-22 14:24:29,052 : INFO : PROGRESS: at sentence #760000, processed 22800000 words, keeping 21503 word types\n",
      "2024-10-22 14:24:29,089 : INFO : PROGRESS: at sentence #770000, processed 23100000 words, keeping 21689 word types\n",
      "2024-10-22 14:24:29,126 : INFO : PROGRESS: at sentence #780000, processed 23400000 words, keeping 21825 word types\n",
      "2024-10-22 14:24:29,164 : INFO : PROGRESS: at sentence #790000, processed 23700000 words, keeping 21963 word types\n",
      "2024-10-22 14:24:29,199 : INFO : PROGRESS: at sentence #800000, processed 24000000 words, keeping 22121 word types\n",
      "2024-10-22 14:24:29,234 : INFO : PROGRESS: at sentence #810000, processed 24300000 words, keeping 22254 word types\n",
      "2024-10-22 14:24:29,268 : INFO : PROGRESS: at sentence #820000, processed 24600000 words, keeping 22400 word types\n",
      "2024-10-22 14:24:29,303 : INFO : PROGRESS: at sentence #830000, processed 24900000 words, keeping 22541 word types\n",
      "2024-10-22 14:24:29,337 : INFO : PROGRESS: at sentence #840000, processed 25200000 words, keeping 22648 word types\n",
      "2024-10-22 14:24:29,372 : INFO : PROGRESS: at sentence #850000, processed 25500000 words, keeping 22765 word types\n",
      "2024-10-22 14:24:29,408 : INFO : PROGRESS: at sentence #860000, processed 25800000 words, keeping 22883 word types\n",
      "2024-10-22 14:24:29,442 : INFO : PROGRESS: at sentence #870000, processed 26100000 words, keeping 22972 word types\n",
      "2024-10-22 14:24:29,477 : INFO : PROGRESS: at sentence #880000, processed 26400000 words, keeping 23060 word types\n",
      "2024-10-22 14:24:29,511 : INFO : PROGRESS: at sentence #890000, processed 26700000 words, keeping 23168 word types\n",
      "2024-10-22 14:24:29,546 : INFO : PROGRESS: at sentence #900000, processed 27000000 words, keeping 23342 word types\n",
      "2024-10-22 14:24:29,581 : INFO : PROGRESS: at sentence #910000, processed 27300000 words, keeping 23465 word types\n",
      "2024-10-22 14:24:29,615 : INFO : PROGRESS: at sentence #920000, processed 27600000 words, keeping 23580 word types\n",
      "2024-10-22 14:24:29,650 : INFO : PROGRESS: at sentence #930000, processed 27900000 words, keeping 23734 word types\n",
      "2024-10-22 14:24:29,685 : INFO : PROGRESS: at sentence #940000, processed 28200000 words, keeping 23861 word types\n",
      "2024-10-22 14:24:29,719 : INFO : PROGRESS: at sentence #950000, processed 28500000 words, keeping 23970 word types\n",
      "2024-10-22 14:24:29,754 : INFO : PROGRESS: at sentence #960000, processed 28800000 words, keeping 24107 word types\n",
      "2024-10-22 14:24:29,789 : INFO : PROGRESS: at sentence #970000, processed 29100000 words, keeping 24252 word types\n",
      "2024-10-22 14:24:29,823 : INFO : PROGRESS: at sentence #980000, processed 29400000 words, keeping 24362 word types\n",
      "2024-10-22 14:24:29,858 : INFO : PROGRESS: at sentence #990000, processed 29700000 words, keeping 24483 word types\n",
      "2024-10-22 14:24:29,893 : INFO : PROGRESS: at sentence #1000000, processed 30000000 words, keeping 24619 word types\n",
      "2024-10-22 14:24:29,927 : INFO : PROGRESS: at sentence #1010000, processed 30300000 words, keeping 24741 word types\n",
      "2024-10-22 14:24:29,961 : INFO : PROGRESS: at sentence #1020000, processed 30600000 words, keeping 24813 word types\n",
      "2024-10-22 14:24:29,996 : INFO : PROGRESS: at sentence #1030000, processed 30900000 words, keeping 24919 word types\n",
      "2024-10-22 14:24:30,030 : INFO : PROGRESS: at sentence #1040000, processed 31200000 words, keeping 25015 word types\n",
      "2024-10-22 14:24:30,067 : INFO : PROGRESS: at sentence #1050000, processed 31500000 words, keeping 25102 word types\n",
      "2024-10-22 14:24:30,102 : INFO : PROGRESS: at sentence #1060000, processed 31800000 words, keeping 25238 word types\n",
      "2024-10-22 14:24:30,136 : INFO : PROGRESS: at sentence #1070000, processed 32100000 words, keeping 25350 word types\n",
      "2024-10-22 14:24:30,171 : INFO : PROGRESS: at sentence #1080000, processed 32400000 words, keeping 25437 word types\n",
      "2024-10-22 14:24:30,198 : INFO : collected 25555 word types from a corpus of 32635320 raw words and 1087844 sentences\n",
      "2024-10-22 14:24:30,198 : INFO : Creating a fresh vocabulary\n",
      "2024-10-22 14:24:30,245 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 25555 unique words (100.00% of original 25555, drops 0)', 'datetime': '2024-10-22T14:24:30.245653', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-10-22 14:24:30,246 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 32635320 word corpus (100.00% of original 32635320, drops 0)', 'datetime': '2024-10-22T14:24:30.246123', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-10-22 14:24:30,305 : INFO : deleting the raw counts dictionary of 25555 items\n",
      "2024-10-22 14:24:30,306 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2024-10-22 14:24:30,306 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 24925141.96127192 word corpus (76.4%% of prior 32635320)', 'datetime': '2024-10-22T14:24:30.306488', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-10-22 14:24:30,382 : INFO : estimated required memory for 25555 words and 150 dimensions: 43443500 bytes\n",
      "2024-10-22 14:24:30,383 : INFO : resetting layer weights\n",
      "2024-10-22 14:24:30,393 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-22T14:24:30.393510', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-10-22 14:24:30,393 : INFO : Word2Vec lifecycle event {'msg': 'training model with 6 workers on 25555 vocabulary and 150 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-10-22T14:24:30.393880', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-10-22 14:24:31,405 : INFO : EPOCH 0 - PROGRESS: at 5.11% examples, 1279119 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:32,411 : INFO : EPOCH 0 - PROGRESS: at 10.59% examples, 1323474 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:33,416 : INFO : EPOCH 0 - PROGRESS: at 16.13% examples, 1340940 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:34,422 : INFO : EPOCH 0 - PROGRESS: at 21.70% examples, 1351699 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:35,427 : INFO : EPOCH 0 - PROGRESS: at 27.37% examples, 1363190 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:36,432 : INFO : EPOCH 0 - PROGRESS: at 32.91% examples, 1366000 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:37,438 : INFO : EPOCH 0 - PROGRESS: at 38.48% examples, 1368631 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:38,440 : INFO : EPOCH 0 - PROGRESS: at 43.99% examples, 1367948 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:39,446 : INFO : EPOCH 0 - PROGRESS: at 49.62% examples, 1372154 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:40,452 : INFO : EPOCH 0 - PROGRESS: at 55.22% examples, 1374895 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:41,452 : INFO : EPOCH 0 - PROGRESS: at 60.61% examples, 1371720 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:42,454 : INFO : EPOCH 0 - PROGRESS: at 66.00% examples, 1368590 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:43,459 : INFO : EPOCH 0 - PROGRESS: at 71.38% examples, 1366750 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:44,462 : INFO : EPOCH 0 - PROGRESS: at 76.99% examples, 1368617 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:45,463 : INFO : EPOCH 0 - PROGRESS: at 82.71% examples, 1371904 words/s, in_qsize 12, out_qsize 0\n",
      "2024-10-22 14:24:46,467 : INFO : EPOCH 0 - PROGRESS: at 88.34% examples, 1373259 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:47,468 : INFO : EPOCH 0 - PROGRESS: at 94.04% examples, 1374612 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:48,473 : INFO : EPOCH 0 - PROGRESS: at 99.76% examples, 1376267 words/s, in_qsize 8, out_qsize 0\n",
      "2024-10-22 14:24:48,507 : INFO : EPOCH 0: training on 32635320 raw words (24927611 effective words) took 18.1s, 1376949 effective words/s\n",
      "2024-10-22 14:24:49,519 : INFO : EPOCH 1 - PROGRESS: at 5.57% examples, 1391769 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:50,528 : INFO : EPOCH 1 - PROGRESS: at 11.23% examples, 1399573 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:51,534 : INFO : EPOCH 1 - PROGRESS: at 16.96% examples, 1406228 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:52,539 : INFO : EPOCH 1 - PROGRESS: at 22.62% examples, 1406882 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:53,541 : INFO : EPOCH 1 - PROGRESS: at 28.07% examples, 1397340 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:54,545 : INFO : EPOCH 1 - PROGRESS: at 33.76% examples, 1400865 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:55,554 : INFO : EPOCH 1 - PROGRESS: at 39.18% examples, 1392451 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:56,566 : INFO : EPOCH 1 - PROGRESS: at 44.97% examples, 1396262 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:57,572 : INFO : EPOCH 1 - PROGRESS: at 50.72% examples, 1400149 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:58,574 : INFO : EPOCH 1 - PROGRESS: at 56.20% examples, 1397770 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:24:59,579 : INFO : EPOCH 1 - PROGRESS: at 61.93% examples, 1399720 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:00,588 : INFO : EPOCH 1 - PROGRESS: at 67.74% examples, 1402122 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:01,593 : INFO : EPOCH 1 - PROGRESS: at 73.47% examples, 1403873 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:02,594 : INFO : EPOCH 1 - PROGRESS: at 79.22% examples, 1405739 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:03,595 : INFO : EPOCH 1 - PROGRESS: at 84.85% examples, 1405346 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:04,598 : INFO : EPOCH 1 - PROGRESS: at 90.67% examples, 1406919 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:05,600 : INFO : EPOCH 1 - PROGRESS: at 96.46% examples, 1407837 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:06,206 : INFO : EPOCH 1: training on 32635320 raw words (24924108 effective words) took 17.7s, 1408946 effective words/s\n",
      "2024-10-22 14:25:07,218 : INFO : EPOCH 2 - PROGRESS: at 5.60% examples, 1399049 words/s, in_qsize 12, out_qsize 0\n",
      "2024-10-22 14:25:08,224 : INFO : EPOCH 2 - PROGRESS: at 11.23% examples, 1401866 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:09,241 : INFO : EPOCH 2 - PROGRESS: at 16.99% examples, 1405389 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:10,244 : INFO : EPOCH 2 - PROGRESS: at 22.77% examples, 1414305 words/s, in_qsize 12, out_qsize 0\n",
      "2024-10-22 14:25:11,245 : INFO : EPOCH 2 - PROGRESS: at 28.53% examples, 1419255 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:12,250 : INFO : EPOCH 2 - PROGRESS: at 34.25% examples, 1420203 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:13,250 : INFO : EPOCH 2 - PROGRESS: at 40.01% examples, 1421817 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:14,251 : INFO : EPOCH 2 - PROGRESS: at 45.70% examples, 1421663 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:15,252 : INFO : EPOCH 2 - PROGRESS: at 51.09% examples, 1413605 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:16,256 : INFO : EPOCH 2 - PROGRESS: at 56.63% examples, 1411122 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:17,263 : INFO : EPOCH 2 - PROGRESS: at 62.35% examples, 1411390 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:18,265 : INFO : EPOCH 2 - PROGRESS: at 68.14% examples, 1413093 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:19,272 : INFO : EPOCH 2 - PROGRESS: at 73.47% examples, 1406155 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:20,275 : INFO : EPOCH 2 - PROGRESS: at 78.70% examples, 1398529 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:21,277 : INFO : EPOCH 2 - PROGRESS: at 83.60% examples, 1386354 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:22,288 : INFO : EPOCH 2 - PROGRESS: at 88.86% examples, 1380310 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:23,289 : INFO : EPOCH 2 - PROGRESS: at 94.28% examples, 1377375 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:24,289 : INFO : EPOCH 2 - PROGRESS: at 99.88% examples, 1377464 words/s, in_qsize 4, out_qsize 1\n",
      "2024-10-22 14:25:24,308 : INFO : EPOCH 2: training on 32635320 raw words (24926721 effective words) took 18.1s, 1377658 effective words/s\n",
      "2024-10-22 14:25:25,319 : INFO : EPOCH 3 - PROGRESS: at 4.99% examples, 1248379 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:26,324 : INFO : EPOCH 3 - PROGRESS: at 9.12% examples, 1139779 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:27,326 : INFO : EPOCH 3 - PROGRESS: at 14.05% examples, 1169678 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:28,328 : INFO : EPOCH 3 - PROGRESS: at 19.56% examples, 1220996 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:29,328 : INFO : EPOCH 3 - PROGRESS: at 25.25% examples, 1260856 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:30,331 : INFO : EPOCH 3 - PROGRESS: at 30.89% examples, 1285663 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:31,335 : INFO : EPOCH 3 - PROGRESS: at 36.58% examples, 1304283 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:32,343 : INFO : EPOCH 3 - PROGRESS: at 42.37% examples, 1319106 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:33,347 : INFO : EPOCH 3 - PROGRESS: at 48.00% examples, 1329047 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:34,352 : INFO : EPOCH 3 - PROGRESS: at 53.69% examples, 1337758 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:35,355 : INFO : EPOCH 3 - PROGRESS: at 59.39% examples, 1345529 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:36,360 : INFO : EPOCH 3 - PROGRESS: at 65.08% examples, 1350757 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:37,367 : INFO : EPOCH 3 - PROGRESS: at 70.77% examples, 1355496 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:38,372 : INFO : EPOCH 3 - PROGRESS: at 76.47% examples, 1359674 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:39,373 : INFO : EPOCH 3 - PROGRESS: at 82.25% examples, 1364543 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:40,375 : INFO : EPOCH 3 - PROGRESS: at 88.01% examples, 1368546 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:41,387 : INFO : EPOCH 3 - PROGRESS: at 93.88% examples, 1371997 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:42,387 : INFO : EPOCH 3 - PROGRESS: at 99.64% examples, 1374533 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:42,444 : INFO : EPOCH 3: training on 32635320 raw words (24927039 effective words) took 18.1s, 1375146 effective words/s\n",
      "2024-10-22 14:25:43,456 : INFO : EPOCH 4 - PROGRESS: at 5.57% examples, 1391745 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:44,458 : INFO : EPOCH 4 - PROGRESS: at 11.30% examples, 1412677 words/s, in_qsize 10, out_qsize 1\n",
      "2024-10-22 14:25:45,459 : INFO : EPOCH 4 - PROGRESS: at 17.08% examples, 1422150 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:46,460 : INFO : EPOCH 4 - PROGRESS: at 22.81% examples, 1424166 words/s, in_qsize 12, out_qsize 0\n",
      "2024-10-22 14:25:47,464 : INFO : EPOCH 4 - PROGRESS: at 28.59% examples, 1427612 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:48,464 : INFO : EPOCH 4 - PROGRESS: at 34.31% examples, 1428141 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:49,474 : INFO : EPOCH 4 - PROGRESS: at 40.13% examples, 1428825 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:50,475 : INFO : EPOCH 4 - PROGRESS: at 45.92% examples, 1430841 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:51,483 : INFO : EPOCH 4 - PROGRESS: at 51.64% examples, 1429810 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:52,484 : INFO : EPOCH 4 - PROGRESS: at 57.37% examples, 1430396 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:53,489 : INFO : EPOCH 4 - PROGRESS: at 63.09% examples, 1429534 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:54,497 : INFO : EPOCH 4 - PROGRESS: at 68.87% examples, 1429265 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:55,498 : INFO : EPOCH 4 - PROGRESS: at 74.63% examples, 1429718 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:56,511 : INFO : EPOCH 4 - PROGRESS: at 80.48% examples, 1429808 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:57,513 : INFO : EPOCH 4 - PROGRESS: at 86.17% examples, 1428826 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:58,526 : INFO : EPOCH 4 - PROGRESS: at 91.92% examples, 1426840 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:59,535 : INFO : EPOCH 4 - PROGRESS: at 97.53% examples, 1423454 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:25:59,948 : INFO : EPOCH 4: training on 32635320 raw words (24925749 effective words) took 17.5s, 1424753 effective words/s\n",
      "2024-10-22 14:26:00,960 : INFO : EPOCH 5 - PROGRESS: at 5.63% examples, 1407785 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:01,980 : INFO : EPOCH 5 - PROGRESS: at 11.30% examples, 1399712 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:02,982 : INFO : EPOCH 5 - PROGRESS: at 17.05% examples, 1410737 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:03,985 : INFO : EPOCH 5 - PROGRESS: at 22.81% examples, 1416665 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:04,988 : INFO : EPOCH 5 - PROGRESS: at 28.59% examples, 1422006 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:05,989 : INFO : EPOCH 5 - PROGRESS: at 34.35% examples, 1424529 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:07,002 : INFO : EPOCH 5 - PROGRESS: at 40.16% examples, 1425195 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:08,002 : INFO : EPOCH 5 - PROGRESS: at 45.86% examples, 1424851 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:09,003 : INFO : EPOCH 5 - PROGRESS: at 51.55% examples, 1424949 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:10,006 : INFO : EPOCH 5 - PROGRESS: at 57.24% examples, 1425037 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:11,010 : INFO : EPOCH 5 - PROGRESS: at 63.06% examples, 1426695 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:12,015 : INFO : EPOCH 5 - PROGRESS: at 68.75% examples, 1425083 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:13,019 : INFO : EPOCH 5 - PROGRESS: at 74.48% examples, 1425043 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:14,020 : INFO : EPOCH 5 - PROGRESS: at 80.17% examples, 1423944 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:15,020 : INFO : EPOCH 5 - PROGRESS: at 85.80% examples, 1422588 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:16,026 : INFO : EPOCH 5 - PROGRESS: at 91.19% examples, 1415884 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:17,030 : INFO : EPOCH 5 - PROGRESS: at 96.70% examples, 1412312 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:17,602 : INFO : EPOCH 5: training on 32635320 raw words (24924578 effective words) took 17.6s, 1412509 effective words/s\n",
      "2024-10-22 14:26:18,609 : INFO : EPOCH 6 - PROGRESS: at 4.99% examples, 1245367 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:19,610 : INFO : EPOCH 6 - PROGRESS: at 9.98% examples, 1247753 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:20,624 : INFO : EPOCH 6 - PROGRESS: at 15.49% examples, 1284418 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:21,633 : INFO : EPOCH 6 - PROGRESS: at 21.15% examples, 1314244 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:22,642 : INFO : EPOCH 6 - PROGRESS: at 26.91% examples, 1336148 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:23,643 : INFO : EPOCH 6 - PROGRESS: at 31.50% examples, 1305850 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:24,643 : INFO : EPOCH 6 - PROGRESS: at 36.43% examples, 1294977 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:25,646 : INFO : EPOCH 6 - PROGRESS: at 42.09% examples, 1308015 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:26,652 : INFO : EPOCH 6 - PROGRESS: at 47.32% examples, 1307798 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:27,654 : INFO : EPOCH 6 - PROGRESS: at 52.50% examples, 1306358 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:28,660 : INFO : EPOCH 6 - PROGRESS: at 57.70% examples, 1305503 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:29,662 : INFO : EPOCH 6 - PROGRESS: at 62.94% examples, 1305269 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:30,672 : INFO : EPOCH 6 - PROGRESS: at 68.38% examples, 1307823 words/s, in_qsize 12, out_qsize 1\n",
      "2024-10-22 14:26:31,673 : INFO : EPOCH 6 - PROGRESS: at 73.74% examples, 1309936 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:32,688 : INFO : EPOCH 6 - PROGRESS: at 79.47% examples, 1316134 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:33,702 : INFO : EPOCH 6 - PROGRESS: at 84.82% examples, 1316036 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:34,707 : INFO : EPOCH 6 - PROGRESS: at 90.61% examples, 1322190 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:35,712 : INFO : EPOCH 6 - PROGRESS: at 96.42% examples, 1327958 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:36,318 : INFO : EPOCH 6: training on 32635320 raw words (24925944 effective words) took 18.7s, 1331942 effective words/s\n",
      "2024-10-22 14:26:37,333 : INFO : EPOCH 7 - PROGRESS: at 5.60% examples, 1395659 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:38,340 : INFO : EPOCH 7 - PROGRESS: at 11.30% examples, 1406809 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:39,346 : INFO : EPOCH 7 - PROGRESS: at 16.71% examples, 1385875 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:40,353 : INFO : EPOCH 7 - PROGRESS: at 21.67% examples, 1347240 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:41,356 : INFO : EPOCH 7 - PROGRESS: at 27.00% examples, 1342879 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:42,360 : INFO : EPOCH 7 - PROGRESS: at 32.54% examples, 1349772 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:43,367 : INFO : EPOCH 7 - PROGRESS: at 38.05% examples, 1352144 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:44,368 : INFO : EPOCH 7 - PROGRESS: at 43.74% examples, 1359603 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:45,370 : INFO : EPOCH 7 - PROGRESS: at 49.41% examples, 1366242 words/s, in_qsize 12, out_qsize 0\n",
      "2024-10-22 14:26:46,372 : INFO : EPOCH 7 - PROGRESS: at 55.07% examples, 1371509 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:47,376 : INFO : EPOCH 7 - PROGRESS: at 60.79% examples, 1375801 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:48,380 : INFO : EPOCH 7 - PROGRESS: at 66.36% examples, 1375664 words/s, in_qsize 12, out_qsize 0\n",
      "2024-10-22 14:26:49,381 : INFO : EPOCH 7 - PROGRESS: at 71.54% examples, 1369567 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:50,389 : INFO : EPOCH 7 - PROGRESS: at 76.93% examples, 1366905 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:51,397 : INFO : EPOCH 7 - PROGRESS: at 82.44% examples, 1366095 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:52,398 : INFO : EPOCH 7 - PROGRESS: at 88.10% examples, 1368661 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:53,398 : INFO : EPOCH 7 - PROGRESS: at 93.55% examples, 1366856 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:54,403 : INFO : EPOCH 7 - PROGRESS: at 99.18% examples, 1367614 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:54,542 : INFO : EPOCH 7: training on 32635320 raw words (24923130 effective words) took 18.2s, 1368268 effective words/s\n",
      "2024-10-22 14:26:55,552 : INFO : EPOCH 8 - PROGRESS: at 5.33% examples, 1333091 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:56,555 : INFO : EPOCH 8 - PROGRESS: at 10.90% examples, 1363177 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:57,557 : INFO : EPOCH 8 - PROGRESS: at 16.62% examples, 1384371 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:58,559 : INFO : EPOCH 8 - PROGRESS: at 21.95% examples, 1370527 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:26:59,563 : INFO : EPOCH 8 - PROGRESS: at 27.61% examples, 1378351 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:00,570 : INFO : EPOCH 8 - PROGRESS: at 33.15% examples, 1378286 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:01,572 : INFO : EPOCH 8 - PROGRESS: at 38.57% examples, 1374462 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:02,572 : INFO : EPOCH 8 - PROGRESS: at 43.96% examples, 1369528 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:03,587 : INFO : EPOCH 8 - PROGRESS: at 49.62% examples, 1373089 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:04,595 : INFO : EPOCH 8 - PROGRESS: at 55.25% examples, 1376176 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:05,595 : INFO : EPOCH 8 - PROGRESS: at 60.92% examples, 1379015 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:06,604 : INFO : EPOCH 8 - PROGRESS: at 66.64% examples, 1381491 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:07,608 : INFO : EPOCH 8 - PROGRESS: at 72.18% examples, 1381460 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:08,627 : INFO : EPOCH 8 - PROGRESS: at 77.35% examples, 1373016 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:09,641 : INFO : EPOCH 8 - PROGRESS: at 82.96% examples, 1373062 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:10,643 : INFO : EPOCH 8 - PROGRESS: at 88.16% examples, 1367910 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:11,651 : INFO : EPOCH 8 - PROGRESS: at 93.88% examples, 1369476 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:12,657 : INFO : EPOCH 8 - PROGRESS: at 99.61% examples, 1371341 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:12,719 : INFO : EPOCH 8: training on 32635320 raw words (24925754 effective words) took 18.2s, 1371928 effective words/s\n",
      "2024-10-22 14:27:13,727 : INFO : EPOCH 9 - PROGRESS: at 5.45% examples, 1365995 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:14,730 : INFO : EPOCH 9 - PROGRESS: at 11.05% examples, 1383577 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:15,733 : INFO : EPOCH 9 - PROGRESS: at 16.53% examples, 1377529 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:16,735 : INFO : EPOCH 9 - PROGRESS: at 22.07% examples, 1378458 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:17,746 : INFO : EPOCH 9 - PROGRESS: at 27.70% examples, 1381180 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:18,750 : INFO : EPOCH 9 - PROGRESS: at 33.18% examples, 1378845 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:19,750 : INFO : EPOCH 9 - PROGRESS: at 38.60% examples, 1375264 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:20,751 : INFO : EPOCH 9 - PROGRESS: at 44.23% examples, 1377786 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:21,754 : INFO : EPOCH 9 - PROGRESS: at 49.68% examples, 1376328 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:22,757 : INFO : EPOCH 9 - PROGRESS: at 55.16% examples, 1375919 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:23,760 : INFO : EPOCH 9 - PROGRESS: at 60.67% examples, 1375101 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:24,768 : INFO : EPOCH 9 - PROGRESS: at 66.24% examples, 1374667 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:25,775 : INFO : EPOCH 9 - PROGRESS: at 71.72% examples, 1373799 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:26,781 : INFO : EPOCH 9 - PROGRESS: at 77.14% examples, 1371578 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:27,789 : INFO : EPOCH 9 - PROGRESS: at 82.62% examples, 1370012 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:28,792 : INFO : EPOCH 9 - PROGRESS: at 88.10% examples, 1369182 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:29,797 : INFO : EPOCH 9 - PROGRESS: at 93.55% examples, 1367020 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:30,806 : INFO : EPOCH 9 - PROGRESS: at 99.00% examples, 1365070 words/s, in_qsize 11, out_qsize 0\n",
      "2024-10-22 14:27:30,977 : INFO : EPOCH 9: training on 32635320 raw words (24925046 effective words) took 18.3s, 1365732 effective words/s\n",
      "2024-10-22 14:27:30,978 : INFO : Word2Vec lifecycle event {'msg': 'training on 326353200 raw words (249255680 effective words) took 180.6s, 1380267 effective words/s', 'datetime': '2024-10-22T14:27:30.978336', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-10-22 14:27:30,978 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=25555, vector_size=150, alpha=0.025>', 'datetime': '2024-10-22T14:27:30.978620', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-10-22 14:27:30,978 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec_model.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-10-22T14:27:30.978965', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'saving'}\n",
      "2024-10-22 14:27:30,979 : INFO : not storing attribute cum_table\n",
      "2024-10-22 14:27:30,988 : INFO : saved word2vec_model.model\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model (classic setting)\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=sequences,  # Tokenized sentences\n",
    "    vector_size=150,             # Dimensionality of word vectors (embedding size)\n",
    "    window=5,                    # Context window size\n",
    "    min_count=1,                 # Ignores words with a frequency lower than this\n",
    "    sg=1,                        # Skip-gram (1) or CBOW (0); classic uses CBOW\n",
    "    workers=6,                   # Number of threads to run in parallel (for performance)\n",
    "    epochs=10                    # Number of iterations (epochs) over the corpus\n",
    ")\n",
    "\n",
    "\n",
    "# Save the model (optional)\n",
    "model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22191,\n",
       " 743,\n",
       " 22641,\n",
       " 14893,\n",
       " 86,\n",
       " 10119,\n",
       " 10031,\n",
       " 24451,\n",
       " 18619,\n",
       " 10472,\n",
       " 11138,\n",
       " 11730,\n",
       " 25460,\n",
       " 10884,\n",
       " 9755,\n",
       " 22171,\n",
       " 1204,\n",
       " 1116,\n",
       " 15011,\n",
       " 10423,\n",
       " 25075,\n",
       " 2961,\n",
       " 22265,\n",
       " 18370,\n",
       " 8453,\n",
       " 10289,\n",
       " 14654,\n",
       " 10314,\n",
       " 19375,\n",
       " 24692,\n",
       " 1674,\n",
       " 15207,\n",
       " 23846,\n",
       " 596,\n",
       " 24727,\n",
       " 10097,\n",
       " 22202,\n",
       " 8738,\n",
       " 11597,\n",
       " 22235,\n",
       " 1775,\n",
       " 1396,\n",
       " 22360,\n",
       " 20278,\n",
       " 11712,\n",
       " 6530,\n",
       " 24863,\n",
       " 13408,\n",
       " 24523,\n",
       " 4562,\n",
       " 14547,\n",
       " 10938,\n",
       " 733,\n",
       " 14707,\n",
       " 22200,\n",
       " 22218,\n",
       " 11760,\n",
       " 15020,\n",
       " 145,\n",
       " 15286,\n",
       " 12631,\n",
       " 12830,\n",
       " 24763,\n",
       " 6277,\n",
       " 6113,\n",
       " 11988,\n",
       " 25252,\n",
       " 12170,\n",
       " 5678,\n",
       " 3000,\n",
       " 24625,\n",
       " 1066,\n",
       " 25493,\n",
       " 9363,\n",
       " 24093,\n",
       " 1013,\n",
       " 16780,\n",
       " 14273,\n",
       " 13987,\n",
       " 19079,\n",
       " 433,\n",
       " 6201,\n",
       " 14897,\n",
       " 22384,\n",
       " 22308,\n",
       " 10736,\n",
       " 9769,\n",
       " 9044,\n",
       " 21055,\n",
       " 22559,\n",
       " 18231,\n",
       " 7612,\n",
       " 12834,\n",
       " 24356,\n",
       " 15125,\n",
       " 22460,\n",
       " 20157,\n",
       " 7626,\n",
       " 24781,\n",
       " 24551,\n",
       " 9286,\n",
       " 1792,\n",
       " 24202,\n",
       " 10049,\n",
       " 12827,\n",
       " 9235,\n",
       " 10298,\n",
       " 22159,\n",
       " 5686,\n",
       " 24953,\n",
       " 15051,\n",
       " 18388,\n",
       " 10999,\n",
       " 22726,\n",
       " 4019,\n",
       " 3085,\n",
       " 10120,\n",
       " 6209,\n",
       " 24768,\n",
       " 14106,\n",
       " 16480,\n",
       " 22388,\n",
       " 18742,\n",
       " 20371,\n",
       " 419,\n",
       " 13142,\n",
       " 1836,\n",
       " 15182,\n",
       " 14441,\n",
       " 9913,\n",
       " 19098,\n",
       " 24512,\n",
       " 1349,\n",
       " 10436,\n",
       " 1135,\n",
       " 22688,\n",
       " 10336,\n",
       " 23206,\n",
       " 15014,\n",
       " 12324,\n",
       " 23910,\n",
       " 14111,\n",
       " 23296,\n",
       " 5097,\n",
       " 19762,\n",
       " 12718,\n",
       " 12797,\n",
       " 7295,\n",
       " 12139,\n",
       " 24894,\n",
       " 24392,\n",
       " 9324,\n",
       " 24212,\n",
       " 22039,\n",
       " 14953,\n",
       " 867,\n",
       " 20356,\n",
       " 8103,\n",
       " 13056,\n",
       " 10172,\n",
       " 15820,\n",
       " 12462,\n",
       " 17408,\n",
       " 1736,\n",
       " 8660,\n",
       " 14108,\n",
       " 25432,\n",
       " 18746,\n",
       " 7879,\n",
       " 13908,\n",
       " 10523,\n",
       " 25495,\n",
       " 2085,\n",
       " 9484,\n",
       " 10057,\n",
       " 13648,\n",
       " 7306,\n",
       " 11775,\n",
       " 8740,\n",
       " 22184,\n",
       " 3066,\n",
       " 5209,\n",
       " 21835,\n",
       " 7860,\n",
       " 13000,\n",
       " 21578,\n",
       " 880,\n",
       " 14468,\n",
       " 22733,\n",
       " 9022,\n",
       " 23135,\n",
       " 13125,\n",
       " 3116,\n",
       " 9107,\n",
       " 809,\n",
       " 14669,\n",
       " 12515,\n",
       " 22424,\n",
       " 15088,\n",
       " 25373,\n",
       " 14980,\n",
       " 7925,\n",
       " 24785,\n",
       " 21791,\n",
       " 24690,\n",
       " 11428,\n",
       " 13393,\n",
       " 14238,\n",
       " 14447,\n",
       " 443,\n",
       " 4610,\n",
       " 8028,\n",
       " 8315,\n",
       " 12313,\n",
       " 17085,\n",
       " 15398,\n",
       " 23457,\n",
       " 18840,\n",
       " 22836,\n",
       " 9881,\n",
       " 7315,\n",
       " 19104,\n",
       " 22298,\n",
       " 8594,\n",
       " 8974,\n",
       " 15201,\n",
       " 13075,\n",
       " 10970,\n",
       " 16148,\n",
       " 9937,\n",
       " 2475,\n",
       " 14385,\n",
       " 21125,\n",
       " 10171,\n",
       " 14024,\n",
       " 16982,\n",
       " 25095,\n",
       " 9659,\n",
       " 13716,\n",
       " 14500,\n",
       " 6630,\n",
       " 23866,\n",
       " 13416,\n",
       " 24393,\n",
       " 283,\n",
       " 9783,\n",
       " 7013,\n",
       " 10712,\n",
       " 2064,\n",
       " 23347,\n",
       " 1842,\n",
       " 17198,\n",
       " 10217,\n",
       " 523,\n",
       " 3811,\n",
       " 4567,\n",
       " 1965,\n",
       " 24461,\n",
       " 6193,\n",
       " 19605,\n",
       " 9972,\n",
       " 23414,\n",
       " 6162,\n",
       " 12027,\n",
       " 10260,\n",
       " 7092,\n",
       " 2534,\n",
       " 18379,\n",
       " 656,\n",
       " 13691,\n",
       " 25393,\n",
       " 22382,\n",
       " 12604,\n",
       " 9122,\n",
       " 22256,\n",
       " 664,\n",
       " 1745,\n",
       " 19037,\n",
       " 25511,\n",
       " 24738,\n",
       " 18326,\n",
       " 20056,\n",
       " 24331,\n",
       " 19498,\n",
       " 20926,\n",
       " 13160,\n",
       " 19249,\n",
       " 9316,\n",
       " 25445,\n",
       " 25107,\n",
       " 13222,\n",
       " 10739,\n",
       " 7873,\n",
       " 6892,\n",
       " 17341,\n",
       " 20912,\n",
       " 6537,\n",
       " 21445,\n",
       " 18697,\n",
       " 9993,\n",
       " 1975,\n",
       " 23812,\n",
       " 25387,\n",
       " 5144,\n",
       " 24827,\n",
       " 15636,\n",
       " 8774,\n",
       " 14084,\n",
       " 14921,\n",
       " 679,\n",
       " 1809,\n",
       " 15089,\n",
       " 22270,\n",
       " 24083,\n",
       " 23030,\n",
       " 22837,\n",
       " 19946,\n",
       " 18657,\n",
       " 22301,\n",
       " 17168,\n",
       " 19781,\n",
       " 15060,\n",
       " 21861,\n",
       " 22751,\n",
       " 19757,\n",
       " 17175,\n",
       " 13958,\n",
       " 17278,\n",
       " 8786,\n",
       " 6118,\n",
       " 24994,\n",
       " 678,\n",
       " 17372,\n",
       " 8062,\n",
       " 3245,\n",
       " 20941,\n",
       " 7321,\n",
       " 24711,\n",
       " 22679,\n",
       " 21114,\n",
       " 25256,\n",
       " 14451,\n",
       " 645,\n",
       " 133,\n",
       " 876,\n",
       " 4044,\n",
       " 22250,\n",
       " 7857,\n",
       " 25116,\n",
       " 19512,\n",
       " 21132,\n",
       " 12866,\n",
       " 9608,\n",
       " 21319,\n",
       " 12340,\n",
       " 3052,\n",
       " 10904,\n",
       " 13076,\n",
       " 7326,\n",
       " 22105,\n",
       " 14310,\n",
       " 5181,\n",
       " 20922,\n",
       " 24481,\n",
       " 17184,\n",
       " 20012,\n",
       " 25169,\n",
       " 15847,\n",
       " 25195,\n",
       " 25399,\n",
       " 11720,\n",
       " 7854,\n",
       " 20364,\n",
       " 6499,\n",
       " 6688,\n",
       " 1132,\n",
       " 15262,\n",
       " 7727,\n",
       " 10317,\n",
       " 1936,\n",
       " 24336,\n",
       " 10099,\n",
       " 20027,\n",
       " 8583,\n",
       " 24777,\n",
       " 1043,\n",
       " 24869,\n",
       " 12435,\n",
       " 18021,\n",
       " 12043,\n",
       " 9803,\n",
       " 20434,\n",
       " 1855,\n",
       " 1944,\n",
       " 3286,\n",
       " 9048,\n",
       " 19632,\n",
       " 17382,\n",
       " 23912,\n",
       " 25192,\n",
       " 1282,\n",
       " 10906,\n",
       " 7594,\n",
       " 15622,\n",
       " 21133,\n",
       " 13049,\n",
       " 18748,\n",
       " 12586,\n",
       " 14265,\n",
       " 7764,\n",
       " 12887,\n",
       " 21569,\n",
       " 14370,\n",
       " 19410,\n",
       " 19659,\n",
       " 3950,\n",
       " 19278,\n",
       " 3727,\n",
       " 17789,\n",
       " 20049,\n",
       " 25189,\n",
       " 10537,\n",
       " 19715,\n",
       " 21837,\n",
       " 12979,\n",
       " 14095,\n",
       " 23133,\n",
       " 3825,\n",
       " 21844,\n",
       " 4537,\n",
       " 20454,\n",
       " 9761,\n",
       " 652,\n",
       " 13736,\n",
       " 12898,\n",
       " 6971,\n",
       " 13132,\n",
       " 930,\n",
       " 14636,\n",
       " 9772,\n",
       " 24545,\n",
       " 15183,\n",
       " 21856,\n",
       " 19500,\n",
       " 1059,\n",
       " 13996,\n",
       " 6139,\n",
       " 4516,\n",
       " 16308,\n",
       " 4831,\n",
       " 7175,\n",
       " 4844,\n",
       " 16214,\n",
       " 8414,\n",
       " 10104,\n",
       " 10395,\n",
       " 2419,\n",
       " 24476,\n",
       " 15390,\n",
       " 15869,\n",
       " 3826,\n",
       " 17554,\n",
       " 5420,\n",
       " 21436,\n",
       " 16304,\n",
       " 14032,\n",
       " 25465,\n",
       " 4078,\n",
       " 19682,\n",
       " 10485,\n",
       " 22063,\n",
       " 5203,\n",
       " 19582,\n",
       " 3392,\n",
       " 23489,\n",
       " 22313,\n",
       " 10176,\n",
       " 16302,\n",
       " 18788,\n",
       " 19319,\n",
       " 12206,\n",
       " 6141,\n",
       " 20438,\n",
       " 5540,\n",
       " 25484,\n",
       " 14552,\n",
       " 7354,\n",
       " 8139,\n",
       " 20163,\n",
       " 23929,\n",
       " 2021,\n",
       " 9124,\n",
       " 15960,\n",
       " 3335,\n",
       " 2342,\n",
       " 883,\n",
       " 13710,\n",
       " 12071,\n",
       " 7696,\n",
       " 23913,\n",
       " 24484,\n",
       " 10245,\n",
       " 25407,\n",
       " 9505,\n",
       " 2430,\n",
       " 15556,\n",
       " 12885,\n",
       " 8602,\n",
       " 4927,\n",
       " 15137,\n",
       " 12523,\n",
       " 24218,\n",
       " 20522,\n",
       " 24475,\n",
       " 8494,\n",
       " 24838,\n",
       " 22041,\n",
       " 4402,\n",
       " 25210,\n",
       " 10846,\n",
       " 14101,\n",
       " 309,\n",
       " 7141,\n",
       " 8047,\n",
       " 6809,\n",
       " 14094,\n",
       " 21983,\n",
       " 25321,\n",
       " 5692,\n",
       " 10070,\n",
       " 7996,\n",
       " 20091,\n",
       " 812,\n",
       " 14137,\n",
       " 7748,\n",
       " 5152,\n",
       " 5028,\n",
       " 18438,\n",
       " 12522,\n",
       " 19321,\n",
       " 2638,\n",
       " 9292,\n",
       " 7640,\n",
       " 12380,\n",
       " 16470,\n",
       " 23203,\n",
       " 18200,\n",
       " 25173,\n",
       " 21167,\n",
       " 22607,\n",
       " 22212,\n",
       " 23208,\n",
       " 10612,\n",
       " 17298,\n",
       " 1446,\n",
       " 12117,\n",
       " 5295,\n",
       " 8023,\n",
       " 24317,\n",
       " 24311,\n",
       " 15562,\n",
       " 12894,\n",
       " 12428,\n",
       " 6628,\n",
       " 22573,\n",
       " 20681,\n",
       " 1430,\n",
       " 3166,\n",
       " 19468,\n",
       " 15446,\n",
       " 18098,\n",
       " 5105,\n",
       " 12208,\n",
       " 1900,\n",
       " 12920,\n",
       " 13031,\n",
       " 13382,\n",
       " 6451,\n",
       " 150,\n",
       " 22721,\n",
       " 6588,\n",
       " 25066,\n",
       " 666,\n",
       " 14678,\n",
       " 20880,\n",
       " 24555,\n",
       " 20979,\n",
       " 12440,\n",
       " 14565,\n",
       " 9298,\n",
       " 6659,\n",
       " 13367,\n",
       " 17393,\n",
       " 21023,\n",
       " 6219,\n",
       " 24888,\n",
       " 7117,\n",
       " 2218,\n",
       " 8704,\n",
       " 6314,\n",
       " 19096,\n",
       " 4972,\n",
       " 9113,\n",
       " 9144,\n",
       " 12839,\n",
       " 8050,\n",
       " 476,\n",
       " 12699,\n",
       " 14748,\n",
       " 19197,\n",
       " 2935,\n",
       " 11470,\n",
       " 13330,\n",
       " 8436,\n",
       " 10555,\n",
       " 9611,\n",
       " 3445,\n",
       " 23082,\n",
       " 20402,\n",
       " 12089,\n",
       " 3233,\n",
       " 4657,\n",
       " 3342,\n",
       " 19639,\n",
       " 3762,\n",
       " 1449,\n",
       " 17389,\n",
       " 18543,\n",
       " 19478,\n",
       " 20878,\n",
       " 24378,\n",
       " 3634,\n",
       " 22988,\n",
       " 8259,\n",
       " 11646,\n",
       " 12765,\n",
       " 1088,\n",
       " 25091,\n",
       " 22436,\n",
       " 20940,\n",
       " 18557,\n",
       " 13165,\n",
       " 9384,\n",
       " 18751,\n",
       " 15913,\n",
       " 5201,\n",
       " 13587,\n",
       " 16286,\n",
       " 10150,\n",
       " 12484,\n",
       " 7297,\n",
       " 22274,\n",
       " 14380,\n",
       " 7382,\n",
       " 8257,\n",
       " 21325,\n",
       " 3417,\n",
       " 3534,\n",
       " 1377,\n",
       " 9145,\n",
       " 23987,\n",
       " 20907,\n",
       " 8665,\n",
       " 24343,\n",
       " 1735,\n",
       " 13788,\n",
       " 3594,\n",
       " 8033,\n",
       " 9137,\n",
       " 12805,\n",
       " 19124,\n",
       " 23102,\n",
       " 19049,\n",
       " 9983,\n",
       " 16986,\n",
       " 20574,\n",
       " 21357,\n",
       " 24334,\n",
       " 7090,\n",
       " 13342,\n",
       " 23113,\n",
       " 4136,\n",
       " 12187,\n",
       " 12190,\n",
       " 12072,\n",
       " 7540,\n",
       " 20147,\n",
       " 8699,\n",
       " 22719,\n",
       " 1885,\n",
       " 8382,\n",
       " 10335,\n",
       " 13510,\n",
       " 10703,\n",
       " 13146,\n",
       " 772,\n",
       " 3051,\n",
       " 14361,\n",
       " 22296,\n",
       " 20094,\n",
       " 3766,\n",
       " 16736,\n",
       " 1115,\n",
       " 1548,\n",
       " 25001,\n",
       " 2730,\n",
       " 16477,\n",
       " 771,\n",
       " 12703,\n",
       " 22982,\n",
       " 14178,\n",
       " 18097,\n",
       " 14877,\n",
       " 2732,\n",
       " 21568,\n",
       " 11232,\n",
       " 20385,\n",
       " 21759,\n",
       " 19024,\n",
       " 2743,\n",
       " 2004,\n",
       " 14555,\n",
       " 564,\n",
       " 11801,\n",
       " 21996,\n",
       " 10533,\n",
       " 12507,\n",
       " 919,\n",
       " 14151,\n",
       " 13381,\n",
       " 16416,\n",
       " 17144,\n",
       " 13430,\n",
       " 12503,\n",
       " 5073,\n",
       " 2540,\n",
       " 22166,\n",
       " 22841,\n",
       " 500,\n",
       " 20217,\n",
       " 9127,\n",
       " 14579,\n",
       " 25146,\n",
       " 15474,\n",
       " 17499,\n",
       " 17855,\n",
       " 8463,\n",
       " 25478,\n",
       " 14615,\n",
       " 10653,\n",
       " 15396,\n",
       " 10709,\n",
       " 502,\n",
       " 24912,\n",
       " 19010,\n",
       " 15464,\n",
       " 25486,\n",
       " 1866,\n",
       " 15195,\n",
       " 16848,\n",
       " 14475,\n",
       " 20167,\n",
       " 3462,\n",
       " 11092,\n",
       " 10000,\n",
       " 12348,\n",
       " 21021,\n",
       " 6861,\n",
       " 14776,\n",
       " 21153,\n",
       " 19495,\n",
       " 10378,\n",
       " 6686,\n",
       " 12447,\n",
       " 19523,\n",
       " 10850,\n",
       " 12267,\n",
       " 22880,\n",
       " 24583,\n",
       " 6579,\n",
       " 3289,\n",
       " 25109,\n",
       " 11885,\n",
       " 2760,\n",
       " 8819,\n",
       " 24875,\n",
       " 3155,\n",
       " 13626,\n",
       " 5706,\n",
       " 17179,\n",
       " 9970,\n",
       " 9117,\n",
       " 22160,\n",
       " 10822,\n",
       " 19665,\n",
       " 19774,\n",
       " 25498,\n",
       " 9257,\n",
       " 20455,\n",
       " 16311,\n",
       " 4729,\n",
       " 12334,\n",
       " 17081,\n",
       " 22272,\n",
       " 25217,\n",
       " 21973,\n",
       " 10226,\n",
       " 25236,\n",
       " 18310,\n",
       " 7466,\n",
       " 15770,\n",
       " 19869,\n",
       " 3214,\n",
       " 4763,\n",
       " 12351,\n",
       " 1018,\n",
       " 7634,\n",
       " 6726,\n",
       " 4604,\n",
       " 13231,\n",
       " 16440,\n",
       " 2681,\n",
       " 20051,\n",
       " 20384,\n",
       " 10252,\n",
       " 15034,\n",
       " 21497,\n",
       " 22332,\n",
       " 16521,\n",
       " 4811,\n",
       " 16215,\n",
       " 12355,\n",
       " 4632,\n",
       " 7472,\n",
       " 7757,\n",
       " 18408,\n",
       " 10517,\n",
       " 25052,\n",
       " 20357,\n",
       " 13048,\n",
       " 13472,\n",
       " 19712,\n",
       " 2482,\n",
       " 6403,\n",
       " 21972,\n",
       " 16363,\n",
       " 16010,\n",
       " 2271,\n",
       " 24400,\n",
       " 3832,\n",
       " 11969,\n",
       " 21224,\n",
       " 19285,\n",
       " 9066,\n",
       " 25025,\n",
       " 2615,\n",
       " 3,\n",
       " 3404,\n",
       " 20586,\n",
       " 13546,\n",
       " 890,\n",
       " 20708,\n",
       " 23126,\n",
       " 24330,\n",
       " 5713,\n",
       " 22659,\n",
       " 1104,\n",
       " 14457,\n",
       " 15586,\n",
       " 13265,\n",
       " 24571,\n",
       " 18293,\n",
       " 12353,\n",
       " 25153,\n",
       " 300,\n",
       " 6326,\n",
       " 20943,\n",
       " 10206,\n",
       " 22978,\n",
       " 7982,\n",
       " 7812,\n",
       " 22090,\n",
       " 5771,\n",
       " 6639,\n",
       " 1988,\n",
       " 3384,\n",
       " 21108,\n",
       " 2621,\n",
       " 12012,\n",
       " 17790,\n",
       " 19939,\n",
       " 594,\n",
       " 14974,\n",
       " 8954,\n",
       " 14280,\n",
       " 18924,\n",
       " 12467,\n",
       " 1130,\n",
       " 21967,\n",
       " 22266,\n",
       " 10631,\n",
       " 10062,\n",
       " 12508,\n",
       " 24695,\n",
       " 3174,\n",
       " 24732,\n",
       " 17774,\n",
       " 18949,\n",
       " 22116,\n",
       " 19089,\n",
       " 1742,\n",
       " 14386,\n",
       " 5042,\n",
       " 12944,\n",
       " 25231,\n",
       " 3261,\n",
       " 12421,\n",
       " 14659,\n",
       " 10540,\n",
       " 2530,\n",
       " 15982,\n",
       " 17432,\n",
       " 22028,\n",
       " 12364,\n",
       " 19787,\n",
       " 11898,\n",
       " 20092,\n",
       " 16018,\n",
       " 14417,\n",
       " 3887,\n",
       " 1909,\n",
       " 10679,\n",
       " 5787,\n",
       " 9423,\n",
       " 14975,\n",
       " 20526,\n",
       " 11331,\n",
       " 12137,\n",
       " 21600,\n",
       " 624,\n",
       " 19964,\n",
       " 17404,\n",
       " 22443,\n",
       " 21599,\n",
       " 19364,\n",
       " 12732,\n",
       " 6901,\n",
       " 24589,\n",
       " 18603,\n",
       " 3793,\n",
       " 22546,\n",
       " 9574,\n",
       " 25228,\n",
       " 1225,\n",
       " 5245,\n",
       " 13747,\n",
       " 17356,\n",
       " 23884,\n",
       " 19772,\n",
       " 19183,\n",
       " 22269,\n",
       " 25014,\n",
       " 25203,\n",
       " 8417,\n",
       " 10774,\n",
       " 16025,\n",
       " 8530,\n",
       " 12031,\n",
       " 9262,\n",
       " 2697,\n",
       " 3829,\n",
       " 10142,\n",
       " 11761,\n",
       " 18758,\n",
       " 14194,\n",
       " 17187,\n",
       " 8481,\n",
       " 15115,\n",
       " 19913,\n",
       " 10388,\n",
       " 8009,\n",
       " 11029,\n",
       " 415,\n",
       " 19452,\n",
       " 15524,\n",
       " 9571,\n",
       " 12908,\n",
       " 16162,\n",
       " 7586,\n",
       " 8450,\n",
       " 19259,\n",
       " 21444,\n",
       " 1247,\n",
       " 21183,\n",
       " 25157,\n",
       " 15613,\n",
       " 9980,\n",
       " 22695,\n",
       " 24887,\n",
       " 13429,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'aaargh' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m similar_words \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maaargh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(similar_words)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/wet/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/wet/lib/python3.12/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'aaargh' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar(\"aaargh\", topn=5)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelOneHot(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM'):\n",
    "        super(LanguageModelOneHot, self).__init__()\n",
    "\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(vocab_size, hidden_dim, num_layers, batch_first=True) # [batch_size, seq_length, vocab_size] -\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embeds =  F.one_hot(x, num_classes=vocab_size).float()\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        out = out.reshape(-1, out.size(2))\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "class LanguageModelWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM'):\n",
    "        super(LanguageModelWord2Vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        embedding_dim = 150\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/5597 [00:11<1:15:31,  1.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     55\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 57\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[1;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256  \n",
    "num_layers = 2\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "\n",
    "# model = LanguageModelOneHot(vocab_size, hidden_dim, num_layers, rnn_type)\n",
    "model = LanguageModelOneHot(vocab_size, hidden_dim,  num_layers, rnn_type)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader):\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets = targets.view(-1)     \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry , and - another Thousand inside the dormitory . He was looking very around by a splash , and Lee did not showing him Crookshanks and went about to know I had been in a former relief .  The skrewts with Household  Had I want to have an\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_word, max_length=50, random_sampling=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_words = [seed_word]  # List to store generated words\n",
    "    \n",
    "    # Convert seed word to index\n",
    "    seed_idx = torch.tensor([[stoi[seed_word]]]).to(device)  # Shape: (1, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Loop through to generate words\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(seed_idx, hidden)\n",
    "        \n",
    "        # Get the predicted word (highest probability or sample)\n",
    "        output = output.squeeze(1)  # Remove the seq_len dimension (now (1, vocab_size))\n",
    "        if random_sampling:\n",
    "            # Sample from the output distribution\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        else:\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the word with highest probability\n",
    "        \n",
    "        # Convert index back to word\n",
    "        predicted_word = itos[predicted_idx]\n",
    "        \n",
    "        # Append the predicted word to the list\n",
    "        generated_words.append(predicted_word)\n",
    "        \n",
    "        # Set the predicted word as the next input (shape: (1, 1))\n",
    "        seed_idx = torch.tensor([[predicted_idx]]).to(device)\n",
    "        \n",
    "        # Stop if an end-of-sequence token is generated (optional)\n",
    "        if predicted_word == \"<eos>\":  # Assuming \"<eos>\" is the token for end of sentence\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Example usage\n",
    "generated_text = generate_text(model, seed_word=\"Harry\", max_length=50, random_sampling=True)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
