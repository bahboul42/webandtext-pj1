
    -------------------------------------
               Model Summary
    -------------------------------------
    RNN type          : GRU
    Embedding type    : word2vec
    Sequence Length   : 50
    Batch Size        : 10

    ---- Hyperparameters ----
    Hidden Dimension  : 256
    Number of Layers  : 3
    Learning Rate     : 0.0003
    Number of Epochs  : 1
    -------------------------------------

Total number of words: 841889, vocabulary size: 41314
100%|████████████████████████████████████████████████████████████| 841839/841839 [00:04<00:00, 189107.96it/s]
  0%|                                                                   | 80/84184 [00:30<9:02:22,  2.58it/s]
Traceback (most recent call last):
  File "/Users/andreas/Desktop/web&text analytics/projet1/webandtext-pj1/train.py", line 212, in <module>
    train_losses = training_loop(model, dataloader, itos, vocab_size, hidden_dim, num_layers, rnn_type, learning_rate, num_epochs, seq_length)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/andreas/Desktop/web&text analytics/projet1/webandtext-pj1/train.py", line 94, in training_loop
    stargets = [itos[t.item()] for t in targets]  # You might want to keep the same targets as for loss
                     ^^^^^^^^
KeyboardInterrupt
