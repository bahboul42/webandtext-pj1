{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilome/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\") # wikitext-2-raw-v1\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\") # \n",
    "train = dataset['train']['text']\n",
    "valid = dataset['validation']['text']\n",
    "test = dataset['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Tokenize the text using regular expressions\n",
    "def tokenize_with_re(data):\n",
    "    tokenized_sentences = [\n",
    "        re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower()) for sentence in data if sentence.strip()\n",
    "    ]\n",
    "    return tokenized_sentences\n",
    "    \n",
    "\n",
    "# Tokenize each dataset split\n",
    "train_tokenized = tokenize_with_re(train)\n",
    "valid_tokenized = tokenize_with_re(valid)\n",
    "test_tokenized = tokenize_with_re(test)\n",
    "\n",
    "# Step 3: Train the Word2Vec model\n",
    "# Combine train, validation, and test data for training\n",
    "all_tokenized_data = train_tokenized + valid_tokenized + test_tokenized\n",
    "\n",
    "# What is the longest sentence in the dataset?\n",
    "max_sentence_len = max([len(sentence) for sentence in all_tokenized_data])\n",
    "print(f\"Max sentence length: {max_sentence_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 16:26:30,473 : INFO : collecting all words and their counts\n",
      "2024-10-22 16:26:30,474 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-10-22 16:26:30,544 : INFO : PROGRESS: at sentence #10000, processed 705394 words, keeping 37689 word types\n",
      "2024-10-22 16:26:30,593 : INFO : PROGRESS: at sentence #20000, processed 1407572 words, keeping 54637 word types\n",
      "2024-10-22 16:26:30,638 : INFO : collected 66929 word types from a corpus of 2053065 raw words and 29119 sentences\n",
      "2024-10-22 16:26:30,639 : INFO : Creating a fresh vocabulary\n",
      "2024-10-22 16:26:30,704 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 66929 unique words (100.00% of original 66929, drops 0)', 'datetime': '2024-10-22T16:26:30.704318', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-10-22 16:26:30,704 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2053065 word corpus (100.00% of original 2053065, drops 0)', 'datetime': '2024-10-22T16:26:30.704686', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-10-22 16:26:30,804 : INFO : deleting the raw counts dictionary of 66929 items\n",
      "2024-10-22 16:26:30,805 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2024-10-22 16:26:30,805 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1580344.9625542224 word corpus (77.0%% of prior 2053065)', 'datetime': '2024-10-22T16:26:30.805500', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-10-22 16:26:30,965 : INFO : estimated required memory for 66929 words and 150 dimensions: 113779300 bytes\n",
      "2024-10-22 16:26:30,966 : INFO : resetting layer weights\n",
      "2024-10-22 16:26:30,991 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-22T16:26:30.991471', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-10-22 16:26:30,991 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 66929 vocabulary and 150 features, using sg=1 hs=0 sample=0.001 negative=5 window=15 shrink_windows=True', 'datetime': '2024-10-22T16:26:30.991819', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-10-22 16:26:31,995 : INFO : EPOCH 0 - PROGRESS: at 21.54% examples, 336161 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:33,026 : INFO : EPOCH 0 - PROGRESS: at 46.83% examples, 364777 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:34,033 : INFO : EPOCH 0 - PROGRESS: at 72.86% examples, 379868 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:35,052 : INFO : EPOCH 0 - PROGRESS: at 98.15% examples, 383485 words/s, in_qsize 3, out_qsize 1\n",
      "2024-10-22 16:26:35,068 : INFO : EPOCH 0: training on 2053065 raw words (1579514 effective words) took 4.1s, 387642 effective words/s\n",
      "2024-10-22 16:26:36,110 : INFO : EPOCH 1 - PROGRESS: at 23.94% examples, 361077 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:37,126 : INFO : EPOCH 1 - PROGRESS: at 50.23% examples, 386891 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:38,146 : INFO : EPOCH 1 - PROGRESS: at 74.10% examples, 383068 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:39,159 : INFO : EPOCH 1 - PROGRESS: at 98.15% examples, 380813 words/s, in_qsize 3, out_qsize 1\n",
      "2024-10-22 16:26:39,168 : INFO : EPOCH 1: training on 2053065 raw words (1580204 effective words) took 4.1s, 385547 effective words/s\n",
      "2024-10-22 16:26:40,176 : INFO : EPOCH 2 - PROGRESS: at 23.48% examples, 365766 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:41,248 : INFO : EPOCH 2 - PROGRESS: at 46.83% examples, 357137 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:42,260 : INFO : EPOCH 2 - PROGRESS: at 72.73% examples, 374010 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:43,264 : INFO : EPOCH 2 - PROGRESS: at 98.44% examples, 381198 words/s, in_qsize 3, out_qsize 1\n",
      "2024-10-22 16:26:43,307 : INFO : EPOCH 2: training on 2053065 raw words (1580558 effective words) took 4.1s, 382084 effective words/s\n",
      "2024-10-22 16:26:44,354 : INFO : EPOCH 3 - PROGRESS: at 23.94% examples, 359125 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:45,366 : INFO : EPOCH 3 - PROGRESS: at 49.67% examples, 382975 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:46,375 : INFO : EPOCH 3 - PROGRESS: at 74.62% examples, 386763 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:47,388 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 387276 words/s, in_qsize 0, out_qsize 1\n",
      "2024-10-22 16:26:47,389 : INFO : EPOCH 3: training on 2053065 raw words (1579835 effective words) took 4.1s, 387243 effective words/s\n",
      "2024-10-22 16:26:48,416 : INFO : EPOCH 4 - PROGRESS: at 23.94% examples, 366320 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:49,429 : INFO : EPOCH 4 - PROGRESS: at 49.16% examples, 383104 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:50,499 : INFO : EPOCH 4 - PROGRESS: at 73.63% examples, 376943 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:51,500 : INFO : EPOCH 4 - PROGRESS: at 97.73% examples, 377288 words/s, in_qsize 4, out_qsize 1\n",
      "2024-10-22 16:26:51,530 : INFO : EPOCH 4: training on 2053065 raw words (1581001 effective words) took 4.1s, 381947 effective words/s\n",
      "2024-10-22 16:26:52,534 : INFO : EPOCH 5 - PROGRESS: at 22.18% examples, 344065 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:53,592 : INFO : EPOCH 5 - PROGRESS: at 47.24% examples, 363799 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:54,599 : INFO : EPOCH 5 - PROGRESS: at 72.37% examples, 374304 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:55,602 : INFO : EPOCH 5 - PROGRESS: at 97.07% examples, 378964 words/s, in_qsize 5, out_qsize 1\n",
      "2024-10-22 16:26:55,644 : INFO : EPOCH 5: training on 2053065 raw words (1580322 effective words) took 4.1s, 384273 effective words/s\n",
      "2024-10-22 16:26:56,689 : INFO : EPOCH 6 - PROGRESS: at 23.94% examples, 359668 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:57,757 : INFO : EPOCH 6 - PROGRESS: at 50.68% examples, 380526 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:58,772 : INFO : EPOCH 6 - PROGRESS: at 76.51% examples, 386776 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:26:59,661 : INFO : EPOCH 6: training on 2053065 raw words (1580044 effective words) took 4.0s, 393506 effective words/s\n",
      "2024-10-22 16:27:00,698 : INFO : EPOCH 7 - PROGRESS: at 20.27% examples, 304025 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:01,706 : INFO : EPOCH 7 - PROGRESS: at 43.33% examples, 333257 words/s, in_qsize 16, out_qsize 0\n",
      "2024-10-22 16:27:02,717 : INFO : EPOCH 7 - PROGRESS: at 63.77% examples, 328239 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:03,767 : INFO : EPOCH 7 - PROGRESS: at 89.24% examples, 344860 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:04,181 : INFO : EPOCH 7: training on 2053065 raw words (1580497 effective words) took 4.5s, 349876 effective words/s\n",
      "2024-10-22 16:27:05,187 : INFO : EPOCH 8 - PROGRESS: at 23.09% examples, 358523 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:06,218 : INFO : EPOCH 8 - PROGRESS: at 46.83% examples, 364622 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:07,245 : INFO : EPOCH 8 - PROGRESS: at 68.28% examples, 352489 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:08,289 : INFO : EPOCH 8 - PROGRESS: at 90.67% examples, 350422 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:08,629 : INFO : EPOCH 8: training on 2053065 raw words (1580980 effective words) took 4.4s, 355550 effective words/s\n",
      "2024-10-22 16:27:09,640 : INFO : EPOCH 9 - PROGRESS: at 22.63% examples, 349099 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:10,679 : INFO : EPOCH 9 - PROGRESS: at 46.83% examples, 362114 words/s, in_qsize 16, out_qsize 0\n",
      "2024-10-22 16:27:11,692 : INFO : EPOCH 9 - PROGRESS: at 72.37% examples, 375023 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:12,713 : INFO : EPOCH 9 - PROGRESS: at 98.44% examples, 382230 words/s, in_qsize 3, out_qsize 1\n",
      "2024-10-22 16:27:12,755 : INFO : EPOCH 9: training on 2053065 raw words (1580417 effective words) took 4.1s, 383240 effective words/s\n",
      "2024-10-22 16:27:13,794 : INFO : EPOCH 10 - PROGRESS: at 23.94% examples, 361792 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:14,842 : INFO : EPOCH 10 - PROGRESS: at 50.23% examples, 381327 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:15,858 : INFO : EPOCH 10 - PROGRESS: at 74.62% examples, 382341 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:16,803 : INFO : EPOCH 10: training on 2053065 raw words (1580207 effective words) took 4.0s, 390521 effective words/s\n",
      "2024-10-22 16:27:17,824 : INFO : EPOCH 11 - PROGRESS: at 23.94% examples, 368322 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:18,829 : INFO : EPOCH 11 - PROGRESS: at 49.67% examples, 389198 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:19,888 : INFO : EPOCH 11 - PROGRESS: at 74.16% examples, 382059 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:20,869 : INFO : EPOCH 11: training on 2053065 raw words (1579704 effective words) took 4.1s, 388652 effective words/s\n",
      "2024-10-22 16:27:21,901 : INFO : EPOCH 12 - PROGRESS: at 23.94% examples, 364479 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:22,948 : INFO : EPOCH 12 - PROGRESS: at 50.23% examples, 382902 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:23,959 : INFO : EPOCH 12 - PROGRESS: at 76.51% examples, 391590 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:24,861 : INFO : EPOCH 12: training on 2053065 raw words (1580448 effective words) took 4.0s, 396126 effective words/s\n",
      "2024-10-22 16:27:25,896 : INFO : EPOCH 13 - PROGRESS: at 23.94% examples, 363157 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:26,903 : INFO : EPOCH 13 - PROGRESS: at 49.67% examples, 386091 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:27,914 : INFO : EPOCH 13 - PROGRESS: at 74.62% examples, 388571 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:28,855 : INFO : EPOCH 13: training on 2053065 raw words (1580039 effective words) took 4.0s, 395773 effective words/s\n",
      "2024-10-22 16:27:29,859 : INFO : EPOCH 14 - PROGRESS: at 23.94% examples, 374485 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:30,864 : INFO : EPOCH 14 - PROGRESS: at 48.66% examples, 384735 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:31,876 : INFO : EPOCH 14 - PROGRESS: at 74.10% examples, 390148 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:32,839 : INFO : EPOCH 14: training on 2053065 raw words (1580602 effective words) took 4.0s, 396929 effective words/s\n",
      "2024-10-22 16:27:33,853 : INFO : EPOCH 15 - PROGRESS: at 23.89% examples, 370539 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:34,875 : INFO : EPOCH 15 - PROGRESS: at 50.23% examples, 390983 words/s, in_qsize 16, out_qsize 0\n",
      "2024-10-22 16:27:35,881 : INFO : EPOCH 15 - PROGRESS: at 75.22% examples, 392720 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:36,801 : INFO : EPOCH 15: training on 2053065 raw words (1580569 effective words) took 4.0s, 399106 effective words/s\n",
      "2024-10-22 16:27:37,841 : INFO : EPOCH 16 - PROGRESS: at 23.94% examples, 361589 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:38,860 : INFO : EPOCH 16 - PROGRESS: at 49.67% examples, 383181 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:39,865 : INFO : EPOCH 16 - PROGRESS: at 75.71% examples, 392274 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:40,779 : INFO : EPOCH 16: training on 2053065 raw words (1580201 effective words) took 4.0s, 397410 effective words/s\n",
      "2024-10-22 16:27:41,786 : INFO : EPOCH 17 - PROGRESS: at 23.94% examples, 373583 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:42,789 : INFO : EPOCH 17 - PROGRESS: at 49.21% examples, 388482 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:43,812 : INFO : EPOCH 17 - PROGRESS: at 74.62% examples, 391326 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:44,735 : INFO : EPOCH 17: training on 2053065 raw words (1580265 effective words) took 4.0s, 399619 effective words/s\n",
      "2024-10-22 16:27:45,748 : INFO : EPOCH 18 - PROGRESS: at 23.94% examples, 370861 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:46,764 : INFO : EPOCH 18 - PROGRESS: at 49.67% examples, 388480 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:47,795 : INFO : EPOCH 18 - PROGRESS: at 74.69% examples, 387717 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:48,722 : INFO : EPOCH 18: training on 2053065 raw words (1580159 effective words) took 4.0s, 396508 effective words/s\n",
      "2024-10-22 16:27:49,757 : INFO : EPOCH 19 - PROGRESS: at 23.94% examples, 363083 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:50,763 : INFO : EPOCH 19 - PROGRESS: at 49.05% examples, 382748 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:51,776 : INFO : EPOCH 19 - PROGRESS: at 75.22% examples, 391163 words/s, in_qsize 15, out_qsize 0\n",
      "2024-10-22 16:27:52,783 : INFO : EPOCH 19 - PROGRESS: at 100.00% examples, 389354 words/s, in_qsize 0, out_qsize 1\n",
      "2024-10-22 16:27:52,784 : INFO : EPOCH 19: training on 2053065 raw words (1580555 effective words) took 4.1s, 389321 effective words/s\n",
      "2024-10-22 16:27:52,784 : INFO : Word2Vec lifecycle event {'msg': 'training on 41061300 raw words (31606121 effective words) took 81.8s, 386421 effective words/s', 'datetime': '2024-10-22T16:27:52.784454', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-10-22 16:27:52,784 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=66929, vector_size=150, alpha=0.025>', 'datetime': '2024-10-22T16:27:52.784674', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-10-22 16:27:52,819 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'wikitext_small_word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-10-22T16:27:52.819761', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'saving'}\n",
      "2024-10-22 16:27:52,821 : INFO : not storing attribute cum_table\n",
      "2024-10-22 16:27:52,852 : INFO : saved wikitext_small_word2vec.model\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "# Define and train the Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=all_tokenized_data,  # Tokenized sentences\n",
    "    vector_size=150,               # Size of word vectors (embeddings)\n",
    "    window=15,                      # Context window size\n",
    "    min_count=1,                   # Minimum frequency for a word to be included in the model\n",
    "    sg=1,                          # Use CBOW (0) or Skip-gram (1)\n",
    "    workers=8,                     # Number of CPU cores to use\n",
    "    epochs=20                      # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "model.save(\"wikitext_small_word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wallonia', 0.6794160604476929), ('ultratip', 0.6522932052612305), ('slovakia', 0.6315439343452454), ('wallonian', 0.6246206164360046), ('megacharts', 0.6237694025039673), ('flanders', 0.6141849160194397), ('hitparade', 0.6137528419494629), ('vrt', 0.6078399419784546), ('eurochart', 0.6038889288902283), ('ultratop', 0.6013078689575195)]\n"
     ]
    }
   ],
   "source": [
    "# Test the model: Find similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"belgium\", topn=10)\n",
    "print(similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
