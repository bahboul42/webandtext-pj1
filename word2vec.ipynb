{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\") # wikitext-2-raw-v1\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\") # \n",
    "train = dataset['train']['text']\n",
    "valid = dataset['validation']['text']\n",
    "test = dataset['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Tokenize the text using regular expressions\n",
    "def tokenize_with_re(data):\n",
    "    tokenized_sentences = [\n",
    "        re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower()) for sentence in data if sentence.strip()\n",
    "    ]\n",
    "    return tokenized_sentences\n",
    "    \n",
    "\n",
    "# Tokenize each dataset split\n",
    "train_tokenized = tokenize_with_re(train)\n",
    "\n",
    "#!!!!!!!\n",
    "train_tokenized = train_tokenized[:len(train_tokenized)//2]\n",
    "\n",
    "valid_tokenized = tokenize_with_re(valid)\n",
    "test_tokenized = tokenize_with_re(test)\n",
    "\n",
    "# Step 3: Train the Word2Vec model\n",
    "# Combine train, validation, and test data for training\n",
    "all_tokenized_data = train_tokenized + valid_tokenized + test_tokenized\n",
    "\n",
    "# What is the longest sentence in the dataset?\n",
    "max_sentence_len = max([len(sentence) for sentence in all_tokenized_data])\n",
    "print(f\"Max sentence length: {max_sentence_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:15:42,417 : INFO : collecting all words and their counts\n",
      "2024-11-17 15:15:42,418 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-11-17 15:15:42,493 : INFO : PROGRESS: at sentence #10000, processed 705394 words, keeping 37689 word types\n",
      "2024-11-17 15:15:42,532 : INFO : collected 49596 word types from a corpus of 1215298 raw words and 17235 sentences\n",
      "2024-11-17 15:15:42,532 : INFO : Creating a fresh vocabulary\n",
      "2024-11-17 15:15:42,583 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 49596 unique words (100.00% of original 49596, drops 0)', 'datetime': '2024-11-17T15:15:42.583340', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-11-17 15:15:42,583 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 1215298 word corpus (100.00% of original 1215298, drops 0)', 'datetime': '2024-11-17T15:15:42.583748', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-11-17 15:15:42,663 : INFO : deleting the raw counts dictionary of 49596 items\n",
      "2024-11-17 15:15:42,663 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2024-11-17 15:15:42,664 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 935082.3649688456 word corpus (76.9%% of prior 1215298)', 'datetime': '2024-11-17T15:15:42.664185', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-11-17 15:15:42,800 : INFO : estimated required memory for 49596 words and 150 dimensions: 84313200 bytes\n",
      "2024-11-17 15:15:42,801 : INFO : resetting layer weights\n",
      "2024-11-17 15:15:42,822 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-17T15:15:42.822829', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-11-17 15:15:42,823 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 49596 vocabulary and 150 features, using sg=1 hs=0 sample=0.001 negative=5 window=15 shrink_windows=True', 'datetime': '2024-11-17T15:15:42.823203', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'train'}\n",
      "2024-11-17 15:15:43,842 : INFO : EPOCH 0 - PROGRESS: at 28.42% examples, 256677 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:44,886 : INFO : EPOCH 0 - PROGRESS: at 61.88% examples, 281869 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:45,807 : INFO : EPOCH 0: training on 1215298 raw words (934740 effective words) took 3.0s, 313421 effective words/s\n",
      "2024-11-17 15:15:46,813 : INFO : EPOCH 1 - PROGRESS: at 35.20% examples, 320729 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:47,853 : INFO : EPOCH 1 - PROGRESS: at 73.73% examples, 336729 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:48,499 : INFO : EPOCH 1: training on 1215298 raw words (935213 effective words) took 2.7s, 347636 effective words/s\n",
      "2024-11-17 15:15:49,507 : INFO : EPOCH 2 - PROGRESS: at 34.78% examples, 320365 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:50,538 : INFO : EPOCH 2 - PROGRESS: at 73.73% examples, 338170 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:51,219 : INFO : EPOCH 2: training on 1215298 raw words (935828 effective words) took 2.7s, 344397 effective words/s\n",
      "2024-11-17 15:15:52,223 : INFO : EPOCH 3 - PROGRESS: at 31.31% examples, 290979 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:53,223 : INFO : EPOCH 3 - PROGRESS: at 63.46% examples, 297766 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:54,097 : INFO : EPOCH 3: training on 1215298 raw words (935128 effective words) took 2.9s, 325112 effective words/s\n",
      "2024-11-17 15:15:55,105 : INFO : EPOCH 4 - PROGRESS: at 32.19% examples, 297543 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:56,122 : INFO : EPOCH 4 - PROGRESS: at 65.28% examples, 302196 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:57,067 : INFO : EPOCH 4: training on 1215298 raw words (934956 effective words) took 3.0s, 315022 effective words/s\n",
      "2024-11-17 15:15:58,092 : INFO : EPOCH 5 - PROGRESS: at 27.64% examples, 247751 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:59,141 : INFO : EPOCH 5 - PROGRESS: at 66.83% examples, 302983 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:15:59,939 : INFO : EPOCH 5: training on 1215298 raw words (935608 effective words) took 2.9s, 326006 effective words/s\n",
      "2024-11-17 15:16:00,958 : INFO : EPOCH 6 - PROGRESS: at 35.73% examples, 323734 words/s, in_qsize 16, out_qsize 0\n",
      "2024-11-17 15:16:01,971 : INFO : EPOCH 6 - PROGRESS: at 74.48% examples, 342786 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:02,606 : INFO : EPOCH 6: training on 1215298 raw words (934658 effective words) took 2.7s, 350724 effective words/s\n",
      "2024-11-17 15:16:03,613 : INFO : EPOCH 7 - PROGRESS: at 34.78% examples, 320666 words/s, in_qsize 14, out_qsize 1\n",
      "2024-11-17 15:16:04,622 : INFO : EPOCH 7 - PROGRESS: at 72.11% examples, 334323 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:05,288 : INFO : EPOCH 7: training on 1215298 raw words (935565 effective words) took 2.7s, 349070 effective words/s\n",
      "2024-11-17 15:16:06,298 : INFO : EPOCH 8 - PROGRESS: at 34.91% examples, 319904 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:07,338 : INFO : EPOCH 8 - PROGRESS: at 74.39% examples, 339657 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:07,934 : INFO : EPOCH 8: training on 1215298 raw words (934853 effective words) took 2.6s, 353613 effective words/s\n",
      "2024-11-17 15:16:08,973 : INFO : EPOCH 9 - PROGRESS: at 36.39% examples, 325436 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:09,974 : INFO : EPOCH 9 - PROGRESS: at 75.22% examples, 345315 words/s, in_qsize 16, out_qsize 1\n",
      "2024-11-17 15:16:10,588 : INFO : EPOCH 9: training on 1215298 raw words (934934 effective words) took 2.7s, 352495 effective words/s\n",
      "2024-11-17 15:16:11,606 : INFO : EPOCH 10 - PROGRESS: at 35.73% examples, 324399 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:12,608 : INFO : EPOCH 10 - PROGRESS: at 71.15% examples, 329651 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:13,328 : INFO : EPOCH 10: training on 1215298 raw words (935050 effective words) took 2.7s, 341514 effective words/s\n",
      "2024-11-17 15:16:14,331 : INFO : EPOCH 11 - PROGRESS: at 34.78% examples, 321635 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:15,339 : INFO : EPOCH 11 - PROGRESS: at 72.11% examples, 335024 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:15,995 : INFO : EPOCH 11: training on 1215298 raw words (934977 effective words) took 2.7s, 350899 effective words/s\n",
      "2024-11-17 15:16:17,004 : INFO : EPOCH 12 - PROGRESS: at 34.78% examples, 319758 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:18,017 : INFO : EPOCH 12 - PROGRESS: at 73.73% examples, 340712 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:18,667 : INFO : EPOCH 12: training on 1215298 raw words (935386 effective words) took 2.7s, 350330 effective words/s\n",
      "2024-11-17 15:16:19,688 : INFO : EPOCH 13 - PROGRESS: at 35.73% examples, 323305 words/s, in_qsize 16, out_qsize 0\n",
      "2024-11-17 15:16:20,720 : INFO : EPOCH 13 - PROGRESS: at 72.11% examples, 328292 words/s, in_qsize 14, out_qsize 1\n",
      "2024-11-17 15:16:21,353 : INFO : EPOCH 13: training on 1215298 raw words (935250 effective words) took 2.7s, 348494 effective words/s\n",
      "2024-11-17 15:16:22,366 : INFO : EPOCH 14 - PROGRESS: at 36.39% examples, 333467 words/s, in_qsize 14, out_qsize 1\n",
      "2024-11-17 15:16:23,375 : INFO : EPOCH 14 - PROGRESS: at 73.52% examples, 340802 words/s, in_qsize 14, out_qsize 1\n",
      "2024-11-17 15:16:23,992 : INFO : EPOCH 14: training on 1215298 raw words (935473 effective words) took 2.6s, 354660 effective words/s\n",
      "2024-11-17 15:16:24,997 : INFO : EPOCH 15 - PROGRESS: at 35.85% examples, 328841 words/s, in_qsize 14, out_qsize 1\n",
      "2024-11-17 15:16:26,006 : INFO : EPOCH 15 - PROGRESS: at 74.48% examples, 345826 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:26,627 : INFO : EPOCH 15: training on 1215298 raw words (935087 effective words) took 2.6s, 355174 effective words/s\n",
      "2024-11-17 15:16:27,647 : INFO : EPOCH 16 - PROGRESS: at 35.73% examples, 323604 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:28,659 : INFO : EPOCH 16 - PROGRESS: at 74.48% examples, 342834 words/s, in_qsize 16, out_qsize 0\n",
      "2024-11-17 15:16:29,254 : INFO : EPOCH 16: training on 1215298 raw words (934932 effective words) took 2.6s, 356159 effective words/s\n",
      "2024-11-17 15:16:30,270 : INFO : EPOCH 17 - PROGRESS: at 34.78% examples, 317433 words/s, in_qsize 14, out_qsize 1\n",
      "2024-11-17 15:16:31,308 : INFO : EPOCH 17 - PROGRESS: at 74.57% examples, 338977 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:31,944 : INFO : EPOCH 17: training on 1215298 raw words (934871 effective words) took 2.7s, 347769 effective words/s\n",
      "2024-11-17 15:16:32,982 : INFO : EPOCH 18 - PROGRESS: at 35.87% examples, 318112 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:34,003 : INFO : EPOCH 18 - PROGRESS: at 75.27% examples, 341974 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:34,602 : INFO : EPOCH 18: training on 1215298 raw words (934969 effective words) took 2.7s, 351980 effective words/s\n",
      "2024-11-17 15:16:35,607 : INFO : EPOCH 19 - PROGRESS: at 34.78% examples, 321121 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:36,635 : INFO : EPOCH 19 - PROGRESS: at 73.73% examples, 339040 words/s, in_qsize 15, out_qsize 0\n",
      "2024-11-17 15:16:37,281 : INFO : EPOCH 19: training on 1215298 raw words (935406 effective words) took 2.7s, 349541 effective words/s\n",
      "2024-11-17 15:16:37,281 : INFO : Word2Vec lifecycle event {'msg': 'training on 24305960 raw words (18702884 effective words) took 54.5s, 343438 effective words/s', 'datetime': '2024-11-17T15:16:37.281487', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'train'}\n",
      "2024-11-17 15:16:37,281 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=49596, vector_size=150, alpha=0.025>', 'datetime': '2024-11-17T15:16:37.281754', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'created'}\n",
      "2024-11-17 15:16:37,282 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'wikitext_small_word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-11-17T15:16:37.282054', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-i386-64bit', 'event': 'saving'}\n",
      "2024-11-17 15:16:37,282 : INFO : not storing attribute cum_table\n",
      "2024-11-17 15:16:37,319 : INFO : saved wikitext_small_word2vec.model\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "# Define and train the Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=all_tokenized_data,  # Tokenized sentences\n",
    "    vector_size=150,               # Size of word vectors (embeddings)\n",
    "    window=15,                      # Context window size\n",
    "    min_count=1,                   # Minimum frequency for a word to be included in the model\n",
    "    sg=1,                          # Use CBOW (0) or Skip-gram (1)\n",
    "    workers=8,                     # Number of CPU cores to use\n",
    "    epochs=20                      # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "model.save(\"wikitext_small_word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wallonia', 0.6630451679229736), ('flanders', 0.6567059755325317), ('slovakia', 0.6296022534370422), ('ultratip', 0.618915319442749), ('francophone', 0.5871874690055847), ('austria', 0.5835490226745605), ('vrt', 0.5830072164535522), ('france', 0.5758391618728638), ('walloon', 0.5744647979736328), ('eurochart', 0.5743144154548645)]\n"
     ]
    }
   ],
   "source": [
    "# Test the model: Find similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"belgium\", topn=10)\n",
    "print(similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
