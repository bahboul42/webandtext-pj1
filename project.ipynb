{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torcheval.metrics import WordErrorRate, Perplexity\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import gensim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ef02faa3c14a499fd99e7fbb29f7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e52ada4321405c8390a7d1d5c52170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fd35b7425b4260a6ec6b8023ab8c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb789adf34a40a1816e32fc8916a198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4285594b2c64a7b8b89a7606c15a7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cabc3bfa914cb4b38ac3cce34d8918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f1ddbe923d47b2b47d52fd8fca3806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 2053065, vocabulary size: 66929\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train = train_data['train']['text']\n",
    "valid = train_data['validation']['text']\n",
    "test = train_data['test']['text']\n",
    "\n",
    "\n",
    "# Step 2: Tokenize the text using regular expressions\n",
    "def tokenize_with_re(data):\n",
    "    tokenized_sentences = [\n",
    "        re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower()) for sentence in data if sentence.strip()\n",
    "    ]\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each dataset split\n",
    "train_tokenized = tokenize_with_re(train)\n",
    "valid_tokenized = tokenize_with_re(valid)\n",
    "test_tokenized = tokenize_with_re(test)\n",
    "\n",
    "flat_train = [item for sublist in train_tokenized for item in sublist]\n",
    "flat_valid = [item for sublist in valid_tokenized for item in sublist]\n",
    "flat_test = [item for sublist in test_tokenized for item in sublist]\n",
    "\n",
    "flat_total = flat_train + flat_valid + flat_test\n",
    "words_available = sorted(set(flat_total))\n",
    "vocab_size = len(words_available)\n",
    "\n",
    "stoi = {word: i for i, word in enumerate(words_available)}\n",
    "itos = {i: word for i, word in enumerate(words_available)}\n",
    "\n",
    "print(f\"Total number of words: {len(flat_total)}, vocabulary size: {len(words_available)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, stoi):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in tqdm(range(len(data) - seq_length)):\n",
    "        # Extract sequence and target\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+1:i+seq_length+1]\n",
    "        \n",
    "        # Convert each word in the sequence and target to indices using stoi\n",
    "        seq_indices = [stoi.get(word, 0) for word in seq]\n",
    "        target_indices = [stoi.get(word, 0) for word in target]\n",
    "        \n",
    "        # Only add sequences and targets of the desired length\n",
    "        if len(seq_indices) == seq_length and len(target_indices) == seq_length:\n",
    "            sequences.append(seq_indices)\n",
    "            targets.append(target_indices)\n",
    "    \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1679606/1679606 [00:09<00:00, 177240.54it/s]\n",
      "100%|██████████| 175865/175865 [00:00<00:00, 254494.35it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.Word2Vec.load('wikitext_small_word2vec.model')\n",
    "stoi = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "seq_val, target_val = create_sequences(flat_valid, seq_length, stoi)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataset_val = TextDataset(seq_val, target_val)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelWord2Vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.word2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.word2vec.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.word2vec.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 59/52488 [01:04<15:59:10,  1.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/andreas/Desktop/web&text analytics/projet1/webandtext-pj1/project.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#W5sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#W5sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Calculate WER\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#W5sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# For WER and Perplexity, keep the original targets shape\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "model = LanguageModelWord2Vec(vocab_size, hidden_dim, num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Initialize WER and Perplexity for both train and validation\n",
    "wer = WordErrorRate()\n",
    "val_wer = WordErrorRate()\n",
    "train_perplexity = Perplexity()\n",
    "val_perplexity = Perplexity()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    wer = wer.to(device)\n",
    "    val_wer = val_wer.to(device)\n",
    "    train_perplexity = train_perplexity.to(device)\n",
    "    val_perplexity = val_perplexity.to(device)\n",
    "\n",
    "train_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "val_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_wer = 0\n",
    "    hidden = None\n",
    "    \n",
    "    # Initialize metrics for the epoch\n",
    "    wer.reset()\n",
    "    train_perplexity.reset()\n",
    "    \n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets_buffer = targets\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate WER\n",
    "        # For WER and Perplexity, keep the original targets shape\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        spreds = [itos[p.item()] for p in preds]\n",
    "        stargets = [itos[t.item()] for t in targets]  # You might want to keep the same targets as for loss\n",
    "        wer.update(spreds, stargets)\n",
    "        \n",
    "        # Calculate perplexity using the original (2D) targets\n",
    "        if torch.cuda.is_available():\n",
    "            train_perplexity.update(outputs.view(batch_size, seq_length, -1), targets_buffer)\n",
    "        else:\n",
    "            train_perplexity.update(outputs.view(batch_size, seq_length, -1).cpu(), targets_buffer.cpu())  # Pass original target shape\n",
    "        \n",
    "    # Calculate training metrics\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    avg_train_wer = wer.compute().item()\n",
    "    avg_train_perplexity = train_perplexity.compute().item()\n",
    "    \n",
    "    train_losses['crossentropy'].append(avg_train_loss)\n",
    "    train_losses['wer'].append(avg_train_wer)\n",
    "    train_losses['perplexity'].append(avg_train_perplexity)\n",
    "    \n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_total_loss = 0\n",
    "    val_wer.reset()\n",
    "    val_perplexity.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, val_inputs, val_targets in enumerate(tqdm(dataloader_val, total=len(dataloader_val))):\n",
    "\n",
    "            # Only look at a part of the validation set\n",
    "            if j > len(dataloader_val)/3:\n",
    "                break\n",
    "            \n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "            val_outputs, _ = model(val_inputs, None)\n",
    "\n",
    "            # Ensure outputs are float32\n",
    "            val_outputs = val_outputs.float()\n",
    "\n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            val_outputs = val_outputs.view(-1, vocab_size)\n",
    "            val_targets = val_targets.view(-1)\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = criterion(val_outputs, val_targets)\n",
    "            val_total_loss += val_loss.item()\n",
    "\n",
    "            # Validation WER and Perplexity\n",
    "            val_preds = torch.argmax(val_outputs, dim=1)\n",
    "            val_wer.update(val_preds, val_targets)\n",
    "            if torch.cuda.is_available():\n",
    "                val_perplexity.update(val_outputs.view(batch_size, seq_length, -1), val_targets)\n",
    "            else:\n",
    "                val_perplexity.update(val_outputs.view(batch_size, seq_length, -1).cpu(), val_targets.cpu())\n",
    "\n",
    "    # Calculate average validation loss, WER, and perplexity\n",
    "    avg_val_loss = val_total_loss / len(dataloader_val)\n",
    "    avg_val_wer = val_wer.compute().item()\n",
    "    avg_val_perplexity = val_perplexity.compute().item()\n",
    "\n",
    "    val_losses['crossentropy'].append(avg_val_loss)\n",
    "    val_losses['wer'].append(avg_val_wer)\n",
    "    val_losses['perplexity'].append(avg_val_perplexity)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training WER: {avg_train_wer:.4f}, Training Perplexity: {avg_train_perplexity:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation WER: {avg_val_wer:.4f}, Validation Perplexity: {avg_val_perplexity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry , and - another Thousand inside the dormitory . He was looking very around by a splash , and Lee did not showing him Crookshanks and went about to know I had been in a former relief . “ The skrewts with Household … Had I want to have an\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_word, max_length=50, random_sampling=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_words = [seed_word]  # List to store generated words\n",
    "    \n",
    "    # Convert seed word to index\n",
    "    seed_idx = torch.tensor([[stoi[seed_word]]]).to(device)  # Shape: (1, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Loop through to generate words\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(seed_idx, hidden)\n",
    "        \n",
    "        # Get the predicted word (highest probability or sample)\n",
    "        output = output.squeeze(1)  # Remove the seq_len dimension (now (1, vocab_size))\n",
    "        if random_sampling:\n",
    "            # Sample from the output distribution\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        else:\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the word with highest probability\n",
    "        \n",
    "        # Convert index back to word\n",
    "        predicted_word = itos[predicted_idx]\n",
    "        \n",
    "        # Append the predicted word to the list\n",
    "        generated_words.append(predicted_word)\n",
    "        \n",
    "        # Set the predicted word as the next input (shape: (1, 1))\n",
    "        seed_idx = torch.tensor([[predicted_idx]]).to(device)\n",
    "        \n",
    "        # Stop if an end-of-sequence token is generated (optional)\n",
    "        if predicted_word == \"<eos>\":  # Assuming \"<eos>\" is the token for end of sentence\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Example usage\n",
    "generated_text = generate_text(model, seed_word=\"Harry\", max_length=50, random_sampling=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikitext_small_fasttext.model.wv.vectors_ngrams.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/andreas/Desktop/web&text analytics/projet1/webandtext-pj1/project.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fasttext \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mFastText\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mwikitext_small_fasttext.model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stoi \u001b[39m=\u001b[39m {word: idx \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(fasttext\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mindex_to_key)}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreas/Desktop/web%26text%20analytics/projet1/webandtext-pj1/project.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m itos \u001b[39m=\u001b[39m {idx: word \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(fasttext\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mindex_to_key)}\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/models/fasttext.py:637\u001b[0m, in \u001b[0;36mFastText.load\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    619\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a previously saved `FastText` model.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \n\u001b[1;32m    621\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \n\u001b[1;32m    636\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(FastText, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mload(\u001b[39m*\u001b[39margs, rethrow\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/models/word2vec.py:1953\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[1;32m   1935\u001b[0m \n\u001b[1;32m   1936\u001b[0m \u001b[39mSee Also\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1950\u001b[0m \n\u001b[1;32m   1951\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1952\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1953\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(Word2Vec, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mload(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1954\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, Word2Vec):\n\u001b[1;32m   1955\u001b[0m         rethrow \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m compress, subname \u001b[39m=\u001b[39m SaveLoad\u001b[39m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m    485\u001b[0m obj \u001b[39m=\u001b[39m unpickle(fname)\n\u001b[0;32m--> 486\u001b[0m obj\u001b[39m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[1;32m    487\u001b[0m obj\u001b[39m.\u001b[39madd_lifecycle_event(\u001b[39m\"\u001b[39m\u001b[39mloaded\u001b[39m\u001b[39m\"\u001b[39m, fname\u001b[39m=\u001b[39mfname)\n\u001b[1;32m    488\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/models/fasttext.py:641\u001b[0m, in \u001b[0;36mFastText._load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_specials\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    640\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     \u001b[39msuper\u001b[39m(FastText, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_load_specials(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    642\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbucket\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    643\u001b[0m         \u001b[39m# should only exist in one place: the wv subcomponent\u001b[39;00m\n\u001b[1;32m    644\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mbucket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbucket\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/models/word2vec.py:1969\u001b[0m, in \u001b[0;36mWord2Vec._load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1967\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_specials\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1968\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1969\u001b[0m     \u001b[39msuper\u001b[39m(Word2Vec, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_load_specials(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1970\u001b[0m     \u001b[39m# for backward compatibility, add/rearrange properties from prior versions\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mns_exponent\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/utils.py:517\u001b[0m, in \u001b[0;36mSaveLoad._load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    515\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m recursively from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.* with mmap=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, attrib, cfname, mmap)\n\u001b[1;32m    516\u001b[0m     \u001b[39mwith\u001b[39;00m ignore_deprecation_warning():\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attrib)\u001b[39m.\u001b[39m_load_specials(cfname, mmap, compress, subname)\n\u001b[1;32m    519\u001b[0m \u001b[39mfor\u001b[39;00m attrib \u001b[39min\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__numpys\u001b[39m\u001b[39m'\u001b[39m, []):\n\u001b[1;32m    520\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with mmap=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, attrib, subname(fname, attrib), mmap)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/models/fasttext.py:1005\u001b[0m, in \u001b[0;36mFastTextKeyedVectors._load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_specials\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1004\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39msuper\u001b[39m(FastTextKeyedVectors, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_load_specials(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, FastTextKeyedVectors):\n\u001b[1;32m   1007\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLoaded object of type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, not expected FastTextKeyedVectors\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/models/keyedvectors.py:263\u001b[0m, in \u001b[0;36mKeyedVectors._load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_specials\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     \u001b[39msuper\u001b[39m(KeyedVectors, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_load_specials(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdoctags\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    265\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upconvert_old_d2vkv()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/gensim/utils.py:528\u001b[0m, in \u001b[0;36mSaveLoad._load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    526\u001b[0m     val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(subname(fname, attrib))[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    527\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 528\u001b[0m     val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(subname(fname, attrib), mmap_mode\u001b[39m=\u001b[39mmmap)\n\u001b[1;32m    530\u001b[0m \u001b[39mwith\u001b[39;00m ignore_deprecation_warning():\n\u001b[1;32m    531\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, attrib, val)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep_learning/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39m(os_fspath(file), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikitext_small_fasttext.model.wv.vectors_ngrams.npy'"
     ]
    }
   ],
   "source": [
    "fasttext = gensim.models.FastText.load('wikitext_small_fasttext.model')\n",
    "stoi = {word: idx for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "\n",
    "batch_size = 128\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelFastText(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelFastText, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.fasttext = gensim.models.FastText.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.fasttext.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.fasttext.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        # Freeze embedding layer\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256  \n",
    "\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "\n",
    "model = LanguageModelFastText(vocab_size, hidden_dim,  num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader):\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets = targets.view(-1)     \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
