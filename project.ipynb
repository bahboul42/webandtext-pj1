{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilome/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torcheval.metrics import WordErrorRate, Perplexity\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import gensim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 2053065, vocabulary size: 66929\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train = train_data['train']['text']\n",
    "valid = train_data['validation']['text']\n",
    "test = train_data['test']['text']\n",
    "\n",
    "\n",
    "# Step 2: Tokenize the text using regular expressions\n",
    "def tokenize_with_re(data):\n",
    "    tokenized_sentences = [\n",
    "        re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower()) for sentence in data if sentence.strip()\n",
    "    ]\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each dataset split\n",
    "train_tokenized = tokenize_with_re(train)\n",
    "valid_tokenized = tokenize_with_re(valid)\n",
    "test_tokenized = tokenize_with_re(test)\n",
    "\n",
    "flat_train = [item for sublist in train_tokenized for item in sublist]\n",
    "flat_valid = [item for sublist in valid_tokenized for item in sublist]\n",
    "flat_test = [item for sublist in test_tokenized for item in sublist]\n",
    "\n",
    "flat_total = flat_train + flat_valid + flat_test\n",
    "words_available = sorted(set(flat_total))\n",
    "vocab_size = len(words_available)\n",
    "\n",
    "stoi = {word: i for i, word in enumerate(words_available)}\n",
    "itos = {i: word for i, word in enumerate(words_available)}\n",
    "\n",
    "print(f\"Total number of words: {len(flat_total)}, vocabulary size: {len(words_available)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, stoi):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in tqdm(range(len(data) - seq_length)):\n",
    "        # Extract sequence and target\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+1:i+seq_length+1]\n",
    "        \n",
    "        # Convert each word in the sequence and target to indices using stoi\n",
    "        seq_indices = [stoi.get(word, 0) for word in seq]\n",
    "        target_indices = [stoi.get(word, 0) for word in target]\n",
    "        \n",
    "        # Only add sequences and targets of the desired length\n",
    "        if len(seq_indices) == seq_length and len(target_indices) == seq_length:\n",
    "            sequences.append(seq_indices)\n",
    "            targets.append(target_indices)\n",
    "    \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1679606/1679606 [00:08<00:00, 199635.08it/s]\n",
      "100%|██████████| 175865/175865 [00:00<00:00, 262313.67it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.Word2Vec.load('wikitext_small_word2vec.model')\n",
    "stoi = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "seq_val, target_val = create_sequences(flat_valid, seq_length, stoi)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataset_val = TextDataset(seq_val, target_val)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelWord2Vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.word2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.word2vec.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.word2vec.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/52488 [00:23<8:13:22,  1.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m         train_perplexity\u001b[38;5;241m.\u001b[39mupdate(outputs\u001b[38;5;241m.\u001b[39mview(batch_size, seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), targets_buffer)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         train_perplexity\u001b[38;5;241m.\u001b[39mupdate(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, targets_buffer\u001b[38;5;241m.\u001b[39mcpu())  \u001b[38;5;66;03m# Pass original target shape\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Calculate training metrics\u001b[39;00m\n\u001b[1;32m     89\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "model = LanguageModelWord2Vec(vocab_size, hidden_dim, num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Initialize WER and Perplexity for both train and validation\n",
    "wer = WordErrorRate()\n",
    "val_wer = WordErrorRate()\n",
    "train_perplexity = Perplexity()\n",
    "val_perplexity = Perplexity()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    wer = wer.to(device)\n",
    "    val_wer = val_wer.to(device)\n",
    "    train_perplexity = train_perplexity.to(device)\n",
    "    val_perplexity = val_perplexity.to(device)\n",
    "\n",
    "train_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "val_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_wer = 0\n",
    "    hidden = None\n",
    "    \n",
    "    # Initialize metrics for the epoch\n",
    "    wer.reset()\n",
    "    train_perplexity.reset()\n",
    "    \n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets_buffer = targets\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate WER\n",
    "        # For WER and Perplexity, keep the original targets shape\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        spreds = [itos[p.item()] for p in preds]\n",
    "        stargets = [itos[t.item()] for t in targets]  # You might want to keep the same targets as for loss\n",
    "        wer.update(spreds, stargets)\n",
    "        \n",
    "        # Calculate perplexity using the original (2D) targets\n",
    "        if torch.cuda.is_available():\n",
    "            train_perplexity.update(outputs.view(batch_size, seq_length, -1), targets_buffer)\n",
    "        else:\n",
    "            train_perplexity.update(outputs.view(batch_size, seq_length, -1).cpu(), targets_buffer.cpu())  # Pass original target shape\n",
    "        \n",
    "    # Calculate training metrics\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    avg_train_wer = wer.compute().item()\n",
    "    avg_train_perplexity = train_perplexity.compute().item()\n",
    "    \n",
    "    train_losses['crossentropy'].append(avg_train_loss)\n",
    "    train_losses['wer'].append(avg_train_wer)\n",
    "    train_losses['perplexity'].append(avg_train_perplexity)\n",
    "    \n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_total_loss = 0\n",
    "    val_wer.reset()\n",
    "    val_perplexity.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, val_inputs, val_targets in enumerate(tqdm(dataloader_val, total=len(dataloader_val))):\n",
    "\n",
    "            # Only look at a part of the validation set\n",
    "            if j > len(dataloader_val)/3:\n",
    "                break\n",
    "            \n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "            val_outputs, _ = model(val_inputs, None)\n",
    "\n",
    "            # Ensure outputs are float32\n",
    "            val_outputs = val_outputs.float()\n",
    "\n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            val_outputs = val_outputs.view(-1, vocab_size)\n",
    "            val_targets = val_targets.view(-1)\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = criterion(val_outputs, val_targets)\n",
    "            val_total_loss += val_loss.item()\n",
    "\n",
    "            # Validation WER and Perplexity\n",
    "            val_preds = torch.argmax(val_outputs, dim=1)\n",
    "            val_wer.update(val_preds, val_targets)\n",
    "            if torch.cuda.is_available():\n",
    "                val_perplexity.update(val_outputs.view(batch_size, seq_length, -1), val_targets)\n",
    "            else:\n",
    "                val_perplexity.update(val_outputs.view(batch_size, seq_length, -1).cpu(), val_targets.cpu())\n",
    "\n",
    "    # Calculate average validation loss, WER, and perplexity\n",
    "    avg_val_loss = val_total_loss / len(dataloader_val)\n",
    "    avg_val_wer = val_wer.compute().item()\n",
    "    avg_val_perplexity = val_perplexity.compute().item()\n",
    "\n",
    "    val_losses['crossentropy'].append(avg_val_loss)\n",
    "    val_losses['wer'].append(avg_val_wer)\n",
    "    val_losses['perplexity'].append(avg_val_perplexity)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training WER: {avg_train_wer:.4f}, Training Perplexity: {avg_train_perplexity:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation WER: {avg_val_wer:.4f}, Validation Perplexity: {avg_val_perplexity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry , and - another Thousand inside the dormitory . He was looking very around by a splash , and Lee did not showing him Crookshanks and went about to know I had been in a former relief . “ The skrewts with Household … Had I want to have an\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_word, max_length=50, random_sampling=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_words = [seed_word]  # List to store generated words\n",
    "    \n",
    "    # Convert seed word to index\n",
    "    seed_idx = torch.tensor([[stoi[seed_word]]]).to(device)  # Shape: (1, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Loop through to generate words\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(seed_idx, hidden)\n",
    "        \n",
    "        # Get the predicted word (highest probability or sample)\n",
    "        output = output.squeeze(1)  # Remove the seq_len dimension (now (1, vocab_size))\n",
    "        if random_sampling:\n",
    "            # Sample from the output distribution\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        else:\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the word with highest probability\n",
    "        \n",
    "        # Convert index back to word\n",
    "        predicted_word = itos[predicted_idx]\n",
    "        \n",
    "        # Append the predicted word to the list\n",
    "        generated_words.append(predicted_word)\n",
    "        \n",
    "        # Set the predicted word as the next input (shape: (1, 1))\n",
    "        seed_idx = torch.tensor([[predicted_idx]]).to(device)\n",
    "        \n",
    "        # Stop if an end-of-sequence token is generated (optional)\n",
    "        if predicted_word == \"<eos>\":  # Assuming \"<eos>\" is the token for end of sentence\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Example usage\n",
    "generated_text = generate_text(model, seed_word=\"Harry\", max_length=50, random_sampling=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CACACACA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = gensim.models.FastText.load('wikitext_small_fasttext.model')\n",
    "stoi = {word: idx for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "\n",
    "batch_size = 128\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelFastText(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelFastText, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.fasttext = gensim.models.FastText.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.fasttext.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.fasttext.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        # Freeze embedding layer\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256  \n",
    "\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "\n",
    "model = LanguageModelFastText(vocab_size, hidden_dim,  num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader):\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets = targets.view(-1)     \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
