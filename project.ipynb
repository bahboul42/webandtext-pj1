{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 2053065, vocabulary size: 66929\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train = train_data['train']['text']\n",
    "valid = train_data['validation']['text']\n",
    "test = train_data['test']['text']\n",
    "\n",
    "\n",
    "# Step 2: Tokenize the text using regular expressions\n",
    "def tokenize_with_re(data):\n",
    "    tokenized_sentences = [\n",
    "        re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower()) for sentence in data if sentence.strip()\n",
    "    ]\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each dataset split\n",
    "train_tokenized = tokenize_with_re(train)\n",
    "valid_tokenized = tokenize_with_re(valid)\n",
    "test_tokenized = tokenize_with_re(test)\n",
    "\n",
    "flat_train = [item for sublist in train_tokenized for item in sublist]\n",
    "flat_valid = [item for sublist in valid_tokenized for item in sublist]\n",
    "flat_test = [item for sublist in test_tokenized for item in sublist]\n",
    "\n",
    "flat_total = flat_train + flat_valid + flat_test\n",
    "words_available = sorted(set(flat_total))\n",
    "vocab_size = len(words_available)\n",
    "\n",
    "stoi = {word: i for i, word in enumerate(words_available)}\n",
    "itos = {i: word for i, word in enumerate(words_available)}\n",
    "\n",
    "print(f\"Total number of words: {len(flat_total)}, vocabulary size: {len(words_available)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, stoi):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in tqdm(range(len(data) - seq_length)):\n",
    "        # Extract sequence and target\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+1:i+seq_length+1]\n",
    "        \n",
    "        # Convert each word in the sequence and target to indices using stoi\n",
    "        seq_indices = [stoi.get(word, 0) for word in seq]\n",
    "        target_indices = [stoi.get(word, 0) for word in target]\n",
    "        \n",
    "        # Only add sequences and targets of the desired length\n",
    "        if len(seq_indices) == seq_length and len(target_indices) == seq_length:\n",
    "            sequences.append(seq_indices)\n",
    "            targets.append(target_indices)\n",
    "    \n",
    "    return sequences, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 0, 1381, 841, 42069, 14120, 0, 59, 116, 334, 3, 2481, 55, 5, 157, 1431, 1, 0, 120, 1129, 7, 4182, 3896, 293, 54, 17, 2643, 0, 1188, 519, 1, 0, 90, 17, 33, 3689, 1631, 13986, 80, 8, 366, 0, 59, 51, 41530, 11, 90, 17438, 263, 3647]\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.Word2Vec.load('wikitext_small_word2vec.model')\n",
    "stoi = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "dataset = Word2VecDataset(sequences, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelOneHot(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM'):\n",
    "        super(LanguageModelOneHot, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(vocab_size, hidden_dim, num_layers, batch_first=True) # [batch_size, seq_length, vocab_size] -\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embeds =  F.one_hot(x, num_classes=self.vocab_size).float()\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        out = out.reshape(-1, out.size(2))\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "\n",
    "class LanguageModelWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelWord2Vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.word2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.word2vec.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.word2vec.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden\n",
    "    \n",
    "model = LanguageModelWord2Vec(vocab_size, 128, 2, rnn_type='LSTM', word2vec_path='wikitext_small_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256  \n",
    "\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "\n",
    "# model = LanguageModelOneHot(vocab_size, hidden_dim, num_layers, rnn_type)\n",
    "model = LanguageModelWord2Vec(vocab_size, hidden_dim,  num_layers, rnn_type)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader):\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets = targets.view(-1)     \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry , and - another Thousand inside the dormitory . He was looking very around by a splash , and Lee did not showing him Crookshanks and went about to know I had been in a former relief . “ The skrewts with Household … Had I want to have an\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_word, max_length=50, random_sampling=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_words = [seed_word]  # List to store generated words\n",
    "    \n",
    "    # Convert seed word to index\n",
    "    seed_idx = torch.tensor([[stoi[seed_word]]]).to(device)  # Shape: (1, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Loop through to generate words\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(seed_idx, hidden)\n",
    "        \n",
    "        # Get the predicted word (highest probability or sample)\n",
    "        output = output.squeeze(1)  # Remove the seq_len dimension (now (1, vocab_size))\n",
    "        if random_sampling:\n",
    "            # Sample from the output distribution\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        else:\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the word with highest probability\n",
    "        \n",
    "        # Convert index back to word\n",
    "        predicted_word = itos[predicted_idx]\n",
    "        \n",
    "        # Append the predicted word to the list\n",
    "        generated_words.append(predicted_word)\n",
    "        \n",
    "        # Set the predicted word as the next input (shape: (1, 1))\n",
    "        seed_idx = torch.tensor([[predicted_idx]]).to(device)\n",
    "        \n",
    "        # Stop if an end-of-sequence token is generated (optional)\n",
    "        if predicted_word == \"<eos>\":  # Assuming \"<eos>\" is the token for end of sentence\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Example usage\n",
    "generated_text = generate_text(model, seed_word=\"Harry\", max_length=50, random_sampling=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = gensim.models.FastText.load('wikitext_small_fasttext.model')\n",
    "stoi = {word: idx for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "dataset = Word2VecDataset(sequences, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
