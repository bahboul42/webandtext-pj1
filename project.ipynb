{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torcheval.metrics import WordErrorRate, Perplexity\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 2053065, vocabulary size: 66929\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train = train_data['train']['text']\n",
    "valid = train_data['validation']['text']\n",
    "test = train_data['test']['text']\n",
    "\n",
    "\n",
    "# Step 2: Tokenize the text using regular expressions\n",
    "def tokenize_with_re(data):\n",
    "    tokenized_sentences = [\n",
    "        re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower()) for sentence in data if sentence.strip()\n",
    "    ]\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each dataset split\n",
    "train_tokenized = tokenize_with_re(train)\n",
    "valid_tokenized = tokenize_with_re(valid)\n",
    "test_tokenized = tokenize_with_re(test)\n",
    "\n",
    "flat_train = [item for sublist in train_tokenized for item in sublist]\n",
    "flat_valid = [item for sublist in valid_tokenized for item in sublist]\n",
    "flat_test = [item for sublist in test_tokenized for item in sublist]\n",
    "\n",
    "flat_total = flat_train + flat_valid + flat_test\n",
    "words_available = sorted(set(flat_total))\n",
    "vocab_size = len(words_available)\n",
    "\n",
    "stoi = {word: i for i, word in enumerate(words_available)}\n",
    "itos = {i: word for i, word in enumerate(words_available)}\n",
    "\n",
    "print(f\"Total number of words: {len(flat_total)}, vocabulary size: {len(words_available)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, stoi):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in tqdm(range(len(data) - seq_length)):\n",
    "        # Extract sequence and target\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+1:i+seq_length+1]\n",
    "        \n",
    "        # Convert each word in the sequence and target to indices using stoi\n",
    "        seq_indices = [stoi.get(word, 0) for word in seq]\n",
    "        target_indices = [stoi.get(word, 0) for word in target]\n",
    "        \n",
    "        # Only add sequences and targets of the desired length\n",
    "        if len(seq_indices) == seq_length and len(target_indices) == seq_length:\n",
    "            sequences.append(seq_indices)\n",
    "            targets.append(target_indices)\n",
    "    \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1679606/1679606 [00:10<00:00, 160837.52it/s]\n",
      "100%|██████████| 175865/175865 [00:00<00:00, 259472.41it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.Word2Vec.load('wikitext_small_word2vec.model')\n",
    "stoi = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "seq_val, target_val = create_sequences(flat_valid, seq_length, stoi)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataset_val = TextDataset(seq_val, target_val)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelWord2Vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.word2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.word2vec.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.word2vec.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26244 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "target should be a two-dimensional tensor, got shape torch.Size([3200]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     wer\u001b[38;5;241m.\u001b[39mupdate(spreds, stargets)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Calculate perplexity\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mtrain_perplexity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# if i % (len(dataloader) // 3) == 0:\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m#     print(f\"Batch {i}/{len(dataloader)}, Evaluating model on validation set\")\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m#     model.eval()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Calculate training metrics\u001b[39;00m\n\u001b[1;32m    115\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/wet2/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/wet2/lib/python3.12/site-packages/torcheval/metrics/text/perplexity.py:105\u001b[0m, in \u001b[0;36mPerplexity.update\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# pyre-ignore[14]: `update` overrides method defined in `Metric` inconsistently.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     93\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TPerplexity:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Update the metric state with new inputs.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     sum_log_probs, num_total \u001b[38;5;241m=\u001b[39m \u001b[43m_perplexity_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_log_probs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sum_log_probs\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_total\n",
      "File \u001b[0;32m/opt/miniconda3/envs/wet2/lib/python3.12/site-packages/torcheval/metrics/functional/text/perplexity.py:90\u001b[0m, in \u001b[0;36m_perplexity_update\u001b[0;34m(input, target, ignore_index)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_perplexity_update\u001b[39m(\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     68\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     69\u001b[0m     ignore_index: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    Sums the log probabilities of the inputs tokens given the target tokens, and counts\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    the total number of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m            the summed log propabilities 'sum_log_probs' and the number of tokens 'num_total'.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[43m_perplexity_input_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     93\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/wet2/lib/python3.12/site-packages/torcheval/metrics/functional/text/perplexity.py:125\u001b[0m, in \u001b[0;36m_perplexity_input_check\u001b[0;34m(input, target, ignore_index)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_perplexity_input_check\u001b[39m(\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    120\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    121\u001b[0m     ignore_index: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget should be a two-dimensional tensor, got shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m         )\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput should be a three-dimensional tensor, got shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: target should be a two-dimensional tensor, got shape torch.Size([3200])."
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256  \n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "model = LanguageModelWord2Vec(vocab_size, hidden_dim, num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Initialize WER and Perplexity for both train and validation\n",
    "wer = WordErrorRate()\n",
    "val_wer = WordErrorRate()\n",
    "train_perplexity = Perplexity()\n",
    "val_perplexity = Perplexity()\n",
    "\n",
    "train_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "val_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_wer = 0\n",
    "    hidden = None\n",
    "    \n",
    "    # Initialize metrics for the epoch\n",
    "    wer.reset()\n",
    "    train_perplexity.reset()\n",
    "    \n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate WER\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        spreds = [itos[p.item()] for p in preds]\n",
    "        stargets = [itos[t.item()] for t in targets]\n",
    "        wer.update(spreds, stargets)\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        train_perplexity.update(preds, targets)\n",
    "        \n",
    "        # if i % (len(dataloader) // 3) == 0:\n",
    "        #     print(f\"Batch {i}/{len(dataloader)}, Evaluating model on validation set\")\n",
    "        #     model.eval()\n",
    "        #     val_total_loss = 0\n",
    "        #     val_wer.reset()\n",
    "        #     val_perplexity.reset()\n",
    "            \n",
    "        #     with torch.no_grad():\n",
    "        #         for j, (val_inputs, val_targets) in tqdm(enumerate(dataloader_val), total=len(dataloader_val)):\n",
    "        #             if j > len(dataloader_val) / 3:\n",
    "        #                 break\n",
    "\n",
    "        #             val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "        #             val_outputs, _ = model(val_inputs, None)\n",
    "                    \n",
    "        #             val_outputs = val_outputs.view(-1, vocab_size)\n",
    "        #             val_targets = val_targets.view(-1)\n",
    "                    \n",
    "        #             val_loss = criterion(val_outputs, val_targets)\n",
    "        #             val_total_loss += val_loss.item()\n",
    "\n",
    "        #             # Validation WER and Perplexity\n",
    "        #             val_preds = torch.argmax(val_outputs, dim=1)\n",
    "        #             val_wer.update(val_preds, val_targets)\n",
    "        #             val_perplexity.update(val_loss.item())\n",
    "            \n",
    "        #     # Calculate average validation loss, WER, and perplexity\n",
    "        #     avg_val_loss = val_total_loss / len(dataloader_val)\n",
    "        #     avg_val_wer = val_wer.compute().item()\n",
    "        #     avg_val_perplexity = val_perplexity.compute().item()\n",
    "            \n",
    "        #     val_losses['crossentropy'].append(avg_val_loss)\n",
    "        #     val_losses['wer'].append(avg_val_wer)\n",
    "        #     val_losses['perplexity'].append(avg_val_perplexity)\n",
    "            \n",
    "        #     print(f'Validation Loss: {avg_val_loss:.4f}, Validation WER: {avg_val_wer:.4f}, Validation Perplexity: {avg_val_perplexity:.4f}')\n",
    "        #     model.train()\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    avg_train_wer = wer.compute().item()\n",
    "    avg_train_perplexity = train_perplexity.compute().item()\n",
    "    \n",
    "    train_losses['crossentropy'].append(avg_train_loss)\n",
    "    train_losses['wer'].append(avg_train_wer)\n",
    "    train_losses['perplexity'].append(avg_train_perplexity)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f'./model_w2v_{epoch}.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}, WER: {avg_train_wer:.4f}, Perplexity: {avg_train_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry , and - another Thousand inside the dormitory . He was looking very around by a splash , and Lee did not showing him Crookshanks and went about to know I had been in a former relief . “ The skrewts with Household … Had I want to have an\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_word, max_length=50, random_sampling=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_words = [seed_word]  # List to store generated words\n",
    "    \n",
    "    # Convert seed word to index\n",
    "    seed_idx = torch.tensor([[stoi[seed_word]]]).to(device)  # Shape: (1, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Loop through to generate words\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(seed_idx, hidden)\n",
    "        \n",
    "        # Get the predicted word (highest probability or sample)\n",
    "        output = output.squeeze(1)  # Remove the seq_len dimension (now (1, vocab_size))\n",
    "        if random_sampling:\n",
    "            # Sample from the output distribution\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        else:\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the word with highest probability\n",
    "        \n",
    "        # Convert index back to word\n",
    "        predicted_word = itos[predicted_idx]\n",
    "        \n",
    "        # Append the predicted word to the list\n",
    "        generated_words.append(predicted_word)\n",
    "        \n",
    "        # Set the predicted word as the next input (shape: (1, 1))\n",
    "        seed_idx = torch.tensor([[predicted_idx]]).to(device)\n",
    "        \n",
    "        # Stop if an end-of-sequence token is generated (optional)\n",
    "        if predicted_word == \"<eos>\":  # Assuming \"<eos>\" is the token for end of sentence\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Example usage\n",
    "generated_text = generate_text(model, seed_word=\"Harry\", max_length=50, random_sampling=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CACACACA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = gensim.models.FastText.load('wikitext_small_fasttext.model')\n",
    "stoi = {word: idx for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "\n",
    "batch_size = 128\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelFastText(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelFastText, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.fasttext = gensim.models.FastText.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.fasttext.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.fasttext.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        # Freeze embedding layer\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256  \n",
    "\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "\n",
    "model = LanguageModelFastText(vocab_size, hidden_dim,  num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader):\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets = targets.view(-1)     \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
