{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Essential Libraries\n",
    "This cell imports the necessary libraries for the project, sets up the device (GPU/CPU) for computations, and checks for available hardware accelerators like CUDA or MPS.\n",
    "\n",
    "- **numpy**: Used for numerical computations (arrays, mathematical operations).\n",
    "- **torch**: A deep learning framework used for building and training machine learning models.\n",
    "  - `torch.nn`: Submodule for defining neural networks.\n",
    "  - `torch.optim`: Provides optimization algorithms for training.\n",
    "  - `torch.nn.functional`: Contains functions for layers, activations, and other operations.\n",
    "  - `torch.utils.data`: Classes for managing datasets and data loaders.\n",
    "- **torcheval.metrics**: Provides evaluation metrics (e.g., Word Error Rate, Perplexity).\n",
    "- **re**: For text processing and pattern matching using regular expressions.\n",
    "- **tqdm**: Adds a progress bar for long-running loops.\n",
    "- **datasets**: Used to load NLP datasets from the Hugging Face library.\n",
    "- **gensim**: Useful for NLP tasks like training Word2Vec models.\n",
    "\n",
    "Finally, the code determines if a GPU (or other accelerator) is available for computations and falls back to the CPU if none is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilome/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torcheval.metrics import WordErrorRate, Perplexity\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import gensim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the Dataset\n",
    "This cell performs several steps to prepare the dataset for use in a natural language processing (NLP) task:\n",
    "\n",
    "### Step 1: Load the Dataset\n",
    "- The **Wikitext-2-raw-v1** dataset is loaded using Hugging Face's `datasets` library.\n",
    "  - `train`, `validation`, and `test` splits are extracted as lists of text strings.\n",
    "\n",
    "### Step 2: Tokenization with Regular Expressions\n",
    "- A custom function `tokenize_with_re` is defined to tokenize the text.\n",
    "  - It uses regular expressions to extract words containing only alphabetic characters (`\\b[a-zA-Z]+\\b`).\n",
    "  - All words are converted to lowercase for consistency.\n",
    "  - Sentences with only whitespace are ignored.\n",
    "\n",
    "### Step 3: Apply Tokenization\n",
    "- The `tokenize_with_re` function is applied to the train, validation, and test datasets.\n",
    "- After tokenization:\n",
    "  - Each tokenized dataset split is flattened into a single list of words.\n",
    "  - A combined list `flat_total` is created by merging all splits.\n",
    "\n",
    "### Step 4: Build the Vocabulary\n",
    "- A sorted set of unique words (`words_available`) is created from the combined word list.\n",
    "- The vocabulary size is determined as the length of this set.\n",
    "\n",
    "### Step 5: Create Mapping Dictionaries\n",
    "- `stoi` (String-to-Int): Maps each word to its unique index.\n",
    "- `itos` (Int-to-String): Maps each index back to its corresponding word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 2053065, vocabulary size: 66929\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train = train_data['train']['text']\n",
    "valid = train_data['validation']['text']\n",
    "test = train_data['test']['text']\n",
    "\n",
    "\n",
    "# Step 2: Tokenize the text using regular expressions\n",
    "def tokenize_with_re(data):\n",
    "    tokenized_sentences = [\n",
    "        re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower()) for sentence in data if sentence.strip()\n",
    "    ]\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each dataset split\n",
    "train_tokenized = tokenize_with_re(train)\n",
    "valid_tokenized = tokenize_with_re(valid)\n",
    "test_tokenized = tokenize_with_re(test)\n",
    "\n",
    "flat_train = [item for sublist in train_tokenized for item in sublist]\n",
    "flat_valid = [item for sublist in valid_tokenized for item in sublist]\n",
    "flat_test = [item for sublist in test_tokenized for item in sublist]\n",
    "\n",
    "flat_total = flat_train + flat_valid + flat_test\n",
    "words_available = sorted(set(flat_total))\n",
    "vocab_size = len(words_available)\n",
    "\n",
    "stoi = {word: i for i, word in enumerate(words_available)}\n",
    "itos = {i: word for i, word in enumerate(words_available)}\n",
    "\n",
    "print(f\"Total number of words: {len(flat_total)}, vocabulary size: {len(words_available)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Create Sequences for Training\n",
    "This function generates input sequences and their corresponding target sequences from the tokenized data, enabling the preparation of data for training a sequence-based model.\n",
    "\n",
    "### Function: `create_sequences(data, seq_length, stoi)`\n",
    "#### Inputs:\n",
    "- **`data`**: The tokenized and flattened text data (a list of words).\n",
    "- **`seq_length`**: The desired length of each input sequence.\n",
    "- **`stoi`**: The dictionary mapping words to their unique indices.\n",
    "\n",
    "#### Outputs:\n",
    "- **`sequences`**: A list of input sequences, where each sequence is a list of word indices of length `seq_length`.\n",
    "- **`targets`**: A list of target sequences, also of length `seq_length`, which represent the expected output for each input sequence.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Iterate through the dataset**:\n",
    "   - The function iterates through the data, extracting a sliding window of size `seq_length` for both inputs (`seq`) and shifted outputs (`target`).\n",
    "2. **Convert words to indices**:\n",
    "   - Each word in the sequence and target is converted to its corresponding index using the `stoi` dictionary.\n",
    "   - If a word is not found in `stoi`, it defaults to index `0`.\n",
    "3. **Filter sequences by length**:\n",
    "   - Only sequences and targets that match the desired length (`seq_length`) are added to the output lists.\n",
    "\n",
    "#### Additional Notes:\n",
    "- **Progress bar**: The `tqdm` library is used to display the progress of sequence generation, particularly useful for large datasets.\n",
    "- This function is crucial for preparing the data for models like RNNs or Transformers, which require fixed-length sequences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, stoi):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in tqdm(range(len(data) - seq_length)):\n",
    "        # Extract sequence and target\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+1:i+seq_length+1]\n",
    "        \n",
    "        # Convert each word in the sequence and target to indices using stoi\n",
    "        seq_indices = [stoi.get(word, 0) for word in seq]\n",
    "        target_indices = [stoi.get(word, 0) for word in target]\n",
    "        \n",
    "        # Only add sequences and targets of the desired length\n",
    "        if len(seq_indices) == seq_length and len(target_indices) == seq_length:\n",
    "            sequences.append(seq_indices)\n",
    "            targets.append(target_indices)\n",
    "    \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model and Prepare Dataset\n",
    "This cell integrates a pre-trained Word2Vec model for word embeddings, prepares training and validation datasets, and sets up PyTorch DataLoaders for efficient batch processing.\n",
    "\n",
    "### Step 1: Load Pre-trained Word2Vec Model\n",
    "- The Word2Vec model is loaded from a pre-saved file (`wikitext_small_word2vec.model`).\n",
    "- Two mappings are created:\n",
    "  - **`stoi`** (String-to-Index): Maps each word in the Word2Vec vocabulary to its unique index.\n",
    "  - **`itos`** (Index-to-String): Maps each index back to its corresponding word.\n",
    "\n",
    "### Step 2: Create Input and Target Sequences\n",
    "- The `create_sequences` function is used to generate:\n",
    "  - **`sequences`**: Input sequences from the training data.\n",
    "  - **`targets`**: Corresponding target sequences.\n",
    "- Validation sequences (`seq_val`) and targets (`target_val`) are similarly created.\n",
    "- **Sequence length** is set to `50`, meaning each input and target sequence consists of 50 words.\n",
    "\n",
    "### Step 3: Define the `TextDataset` Class\n",
    "- A custom PyTorch dataset class is created to handle sequences and targets:\n",
    "  - **`__init__`**: Stores the sequences and targets.\n",
    "  - **`__len__`**: Returns the total number of sequences.\n",
    "  - **`__getitem__`**: Retrieves a specific sequence and its corresponding target, converting them to PyTorch tensors.\n",
    "\n",
    "### Step 4: Set Up DataLoaders\n",
    "- PyTorch DataLoaders are created for both training and validation datasets:\n",
    "  - **Batch size** is set to `32`.\n",
    "  - **Shuffling** is enabled for training to ensure randomization during training.\n",
    "  - Validation DataLoader does not shuffle the data, as it is used for evaluation.\n",
    "\n",
    "### Purpose:\n",
    "- This setup enables efficient training and validation of sequence models using batches, with pre-trained word embeddings providing meaningful representations for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1679606/1679606 [00:08<00:00, 199635.08it/s]\n",
      "100%|██████████| 175865/175865 [00:00<00:00, 262313.67it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.Word2Vec.load('wikitext_small_word2vec.model')\n",
    "stoi = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "seq_val, target_val = create_sequences(flat_valid, seq_length, stoi)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataset_val = TextDataset(seq_val, target_val)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model with Pre-trained Word2Vec Embeddings\n",
    "This class defines a language model using PyTorch, incorporating pre-trained Word2Vec embeddings for word representation and RNN-based architectures for sequence modeling.\n",
    "\n",
    "### Class: `LanguageModelWord2Vec`\n",
    "#### Inputs:\n",
    "- **`vocab_size`**: Size of the vocabulary (number of unique words).\n",
    "- **`hidden_dim`**: Number of hidden units in the RNN.\n",
    "- **`num_layers`**: Number of layers in the RNN.\n",
    "- **`rnn_type`**: Type of RNN to use (`'LSTM'` or `'GRU'`).\n",
    "- **`word2vec_path`**: Path to the pre-trained Word2Vec model file.\n",
    "\n",
    "#### Key Components:\n",
    "1. **Load Word2Vec Embeddings**:\n",
    "   - A pre-trained Gensim Word2Vec model is loaded.\n",
    "   - The embedding dimension is inferred from the Word2Vec model.\n",
    "\n",
    "2. **Embedding Layer Initialization**:\n",
    "   - The embedding layer is initialized with the pre-trained Word2Vec weights.\n",
    "   - The weights are frozen (`requires_grad=False`) to prevent updates during training.\n",
    "\n",
    "3. **Recurrent Neural Network (RNN)**:\n",
    "   - An RNN (either LSTM or GRU) is initialized.\n",
    "   - Takes input word embeddings and outputs a sequence of hidden states.\n",
    "\n",
    "4. **Fully Connected Layer**:\n",
    "   - A linear layer maps the RNN’s hidden state outputs to vocabulary size, producing logits for each word in the vocabulary.\n",
    "\n",
    "#### Forward Method:\n",
    "- **Input**: \n",
    "  - `x`: Tensor of word indices with shape `(batch_size, seq_length)`.\n",
    "  - `hidden`: Initial hidden states for the RNN.\n",
    "- **Steps**:\n",
    "  1. Convert word indices to embeddings using the pre-trained embedding layer.\n",
    "     - Output shape: `(batch_size, seq_length, embedding_dim)`.\n",
    "  2. Pass embeddings through the RNN.\n",
    "     - Output shape: `(batch_size, seq_length, hidden_dim)`.\n",
    "  3. Reshape RNN output and apply the fully connected layer.\n",
    "     - Final output shape: `(batch_size * seq_length, vocab_size)`.\n",
    "- **Output**:\n",
    "  - Predictions (logits) for each word in the vocabulary.\n",
    "  - Updated hidden states for the RNN.\n",
    "\n",
    "### Purpose:\n",
    "This model combines the semantic richness of pre-trained Word2Vec embeddings with the sequential modeling power of RNNs, making it suitable for NLP tasks like next-word prediction or language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelWord2Vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.word2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.word2vec.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.word2vec.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation of the Language Model\n",
    "This cell initializes and trains the `LanguageModelWord2Vec` class using pre-trained Word2Vec embeddings. It also evaluates the model on a validation dataset during training to monitor performance.\n",
    "\n",
    "### Key Steps:\n",
    "#### 1. **Initialize the Model**\n",
    "- **Parameters**:\n",
    "  - `hidden_dim`: Number of hidden units in the RNN.\n",
    "  - `num_layers`: Number of RNN layers.\n",
    "  - `rnn_type`: RNN architecture (`'LSTM'` or `'GRU'`).\n",
    "- The model is initialized with the vocabulary size and moved to the appropriate device (CPU/GPU).\n",
    "\n",
    "#### 2. **Define Loss Function and Optimizer**\n",
    "- **Loss Function**: Cross-entropy loss is used to calculate the difference between predicted and actual word indices.\n",
    "- **Optimizer**: Adam optimizer is used with a learning rate of 0.0003.\n",
    "\n",
    "#### 3. **Initialize Metrics**\n",
    "- Word Error Rate (WER) and Perplexity are initialized for both training and validation to evaluate the model:\n",
    "  - **WER**: Measures the discrepancy between predicted and target word sequences.\n",
    "  - **Perplexity**: Evaluates the probability of the target sequence under the model's predictions.\n",
    "\n",
    "#### 4. **Training Loop**\n",
    "- The training is conducted for a specified number of epochs (`num_epochs`):\n",
    "  - **Forward Pass**: The model processes input sequences and predicts outputs.\n",
    "  - **Loss Calculation**: Cross-entropy loss is computed between predictions and targets.\n",
    "  - **Backpropagation**: Gradients are computed and applied to update the model weights.\n",
    "  - **Metric Calculation**:\n",
    "    - WER is updated using predicted and actual sequences.\n",
    "    - Perplexity is computed using predictions and targets.\n",
    "\n",
    "#### 5. **Validation Phase**\n",
    "- After each epoch, the model is evaluated on the validation dataset:\n",
    "  - Predictions are generated, and validation loss, WER, and perplexity are calculated.\n",
    "  - The validation metrics are stored for analysis.\n",
    "\n",
    "#### 6. **Performance Logging**\n",
    "- For each epoch, the following metrics are printed:\n",
    "  - Training loss, WER, and perplexity.\n",
    "  - Validation loss, WER, and perplexity.\n",
    "\n",
    "### Purpose:\n",
    "This training loop aims to optimize the language model's performance on the training data while monitoring its generalization to unseen validation data. The use of metrics like WER and perplexity provides insights into the model's accuracy and language fluency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/52488 [00:23<8:13:22,  1.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m         train_perplexity\u001b[38;5;241m.\u001b[39mupdate(outputs\u001b[38;5;241m.\u001b[39mview(batch_size, seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), targets_buffer)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         train_perplexity\u001b[38;5;241m.\u001b[39mupdate(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, targets_buffer\u001b[38;5;241m.\u001b[39mcpu())  \u001b[38;5;66;03m# Pass original target shape\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Calculate training metrics\u001b[39;00m\n\u001b[1;32m     89\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "model = LanguageModelWord2Vec(vocab_size, hidden_dim, num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Initialize WER and Perplexity for both train and validation\n",
    "wer = WordErrorRate()\n",
    "val_wer = WordErrorRate()\n",
    "train_perplexity = Perplexity()\n",
    "val_perplexity = Perplexity()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    wer = wer.to(device)\n",
    "    val_wer = val_wer.to(device)\n",
    "    train_perplexity = train_perplexity.to(device)\n",
    "    val_perplexity = val_perplexity.to(device)\n",
    "\n",
    "train_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "val_losses = {'crossentropy': [], 'wer': [], 'perplexity': []}\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_wer = 0\n",
    "    hidden = None\n",
    "    \n",
    "    # Initialize metrics for the epoch\n",
    "    wer.reset()\n",
    "    train_perplexity.reset()\n",
    "    \n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets_buffer = targets\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate WER\n",
    "        # For WER and Perplexity, keep the original targets shape\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        spreds = [itos[p.item()] for p in preds]\n",
    "        stargets = [itos[t.item()] for t in targets]  # You might want to keep the same targets as for loss\n",
    "        wer.update(spreds, stargets)\n",
    "        \n",
    "        # Calculate perplexity using the original (2D) targets\n",
    "        if torch.cuda.is_available():\n",
    "            train_perplexity.update(outputs.view(batch_size, seq_length, -1), targets_buffer)\n",
    "        else:\n",
    "            train_perplexity.update(outputs.view(batch_size, seq_length, -1).cpu(), targets_buffer.cpu())  # Pass original target shape\n",
    "        \n",
    "    # Calculate training metrics\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    avg_train_wer = wer.compute().item()\n",
    "    avg_train_perplexity = train_perplexity.compute().item()\n",
    "    \n",
    "    train_losses['crossentropy'].append(avg_train_loss)\n",
    "    train_losses['wer'].append(avg_train_wer)\n",
    "    train_losses['perplexity'].append(avg_train_perplexity)\n",
    "    \n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_total_loss = 0\n",
    "    val_wer.reset()\n",
    "    val_perplexity.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, val_inputs, val_targets in enumerate(tqdm(dataloader_val, total=len(dataloader_val))):\n",
    "\n",
    "            # Only look at a part of the validation set\n",
    "            if j > len(dataloader_val)/3:\n",
    "                break\n",
    "            \n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "            val_outputs, _ = model(val_inputs, None)\n",
    "\n",
    "            # Ensure outputs are float32\n",
    "            val_outputs = val_outputs.float()\n",
    "\n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            val_outputs = val_outputs.view(-1, vocab_size)\n",
    "            val_targets = val_targets.view(-1)\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = criterion(val_outputs, val_targets)\n",
    "            val_total_loss += val_loss.item()\n",
    "\n",
    "            # Validation WER and Perplexity\n",
    "            val_preds = torch.argmax(val_outputs, dim=1)\n",
    "            val_wer.update(val_preds, val_targets)\n",
    "            if torch.cuda.is_available():\n",
    "                val_perplexity.update(val_outputs.view(batch_size, seq_length, -1), val_targets)\n",
    "            else:\n",
    "                val_perplexity.update(val_outputs.view(batch_size, seq_length, -1).cpu(), val_targets.cpu())\n",
    "\n",
    "    # Calculate average validation loss, WER, and perplexity\n",
    "    avg_val_loss = val_total_loss / len(dataloader_val)\n",
    "    avg_val_wer = val_wer.compute().item()\n",
    "    avg_val_perplexity = val_perplexity.compute().item()\n",
    "\n",
    "    val_losses['crossentropy'].append(avg_val_loss)\n",
    "    val_losses['wer'].append(avg_val_wer)\n",
    "    val_losses['perplexity'].append(avg_val_perplexity)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training WER: {avg_train_wer:.4f}, Training Perplexity: {avg_train_perplexity:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation WER: {avg_val_wer:.4f}, Validation Perplexity: {avg_val_perplexity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry , and - another Thousand inside the dormitory . He was looking very around by a splash , and Lee did not showing him Crookshanks and went about to know I had been in a former relief . “ The skrewts with Household … Had I want to have an\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_word, max_length=50, random_sampling=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_words = [seed_word]  # List to store generated words\n",
    "    \n",
    "    # Convert seed word to index\n",
    "    seed_idx = torch.tensor([[stoi[seed_word]]]).to(device)  # Shape: (1, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Loop through to generate words\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(seed_idx, hidden)\n",
    "        \n",
    "        # Get the predicted word (highest probability or sample)\n",
    "        output = output.squeeze(1)  # Remove the seq_len dimension (now (1, vocab_size))\n",
    "        if random_sampling:\n",
    "            # Sample from the output distribution\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        else:\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the word with highest probability\n",
    "        \n",
    "        # Convert index back to word\n",
    "        predicted_word = itos[predicted_idx]\n",
    "        \n",
    "        # Append the predicted word to the list\n",
    "        generated_words.append(predicted_word)\n",
    "        \n",
    "        # Set the predicted word as the next input (shape: (1, 1))\n",
    "        seed_idx = torch.tensor([[predicted_idx]]).to(device)\n",
    "        \n",
    "        # Stop if an end-of-sequence token is generated (optional)\n",
    "        if predicted_word == \"<eos>\":  # Assuming \"<eos>\" is the token for end of sentence\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Example usage\n",
    "generated_text = generate_text(model, seed_word=\"Harry\", max_length=50, random_sampling=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "This section replicates what has been done with Word2Vec and applies it to FastText.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = gensim.models.FastText.load('wikitext_small_fasttext.model')\n",
    "stoi = {word: idx for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "itos = {idx: word for idx, word in enumerate(fasttext.wv.index_to_key)}\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 50\n",
    "sequences, targets = create_sequences(flat_train, seq_length, stoi)\n",
    "\n",
    "batch_size = 128\n",
    "dataset = TextDataset(sequences, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelFastText(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, rnn_type='LSTM',  word2vec_path='wikitext_small_word2vec.model'):\n",
    "        super(LanguageModelFastText, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Step 1: Load the Gensim Word2Vec model\n",
    "        self.fasttext = gensim.models.FastText.load(word2vec_path)\n",
    "        \n",
    "        # Get the embedding dimensions from the Gensim model\n",
    "        embedding_dim = self.fasttext.wv.vector_size\n",
    "\n",
    "        # Step 2: Initialize the embedding layer with pretrained Word2Vec weights\n",
    "        \n",
    "        weights = torch.FloatTensor(self.fasttext.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        \n",
    "        # Freeze embedding layer\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Step 3: Initialize RNN (LSTM or GRU)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_dim)\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        # out shape: (batch_size * seq_length, hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # out shape: (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 256  \n",
    "\n",
    "num_layers = 3\n",
    "rnn_type = 'LSTM' # Choose 'LSTM' or 'GRU'\n",
    "\n",
    "model = LanguageModelFastText(vocab_size, hidden_dim,  num_layers, rnn_type)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader):\n",
    "        hidden = None\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states to prevent backpropagating through the entire training history\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        outputs = outputs.view(-1, vocab_size) \n",
    "        targets = targets.view(-1)     \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
